Here we recall the basic concepts from abstract algebra that
constitute the background for all the chapters that follow. To the
reader interested in reading more about these topics, we recommend
\cite{lang} for general algebra, \cite{lidl+niederreiter:2} for finite
fields and \cite{silverman:elliptic,silverman:advanced} for elliptic
curves.

\section{Groups, Rings, Modules}
\label{sec:ring-fields}

\subsection{Objects}
\label{sec:ring-fields:objects}

\subsubsection{Groups}

A \index{group}\textbf{group} is a pair $(G,\cdot)$ such that $G$ is a
set and $\cdot:G\times G\ra G$ is an \emph{internal composition law}
satisfying:
\begin{itemize}
\item \index{associativity}\textbf{Associativity}: $(a\cdot b)\cdot c
  = a \cdot (b\cdot c)$ for any $a,b\in G$;
\item There is an element $e\in G$, called the
  \index{identity~element}\textbf{identity}, such that $a\cdot e =
  e\cdot a = a$ for any $a\in G$;
\item For any $a\in G$ there is an
  \index{inverse~element}\textbf{inverse element} $a^{-1}$ such that
  $a\cdot a^{-1} = a^{-1}\cdot a = e$.
\end{itemize}
If $\cdot$ also satisfies $a\cdot b=b\cdot a$
(\index{commutativity}\textbf{commutativity}), the group is said to be
\index{group!abelian}\textbf{abelian}. The group composed of one
single element with the obvious law is called the
\index{group!trivial}\textbf{trivial group}.

A \index{subgroup}\textbf{subgroup} of a group $(G,\cdot)$ is a group
$(H,\circ)$ such that $H\subset G$ and $\circ$ is the restriction of
$\cdot$ to $H$. Any group has two trivial subgroups: the trivial group
and itself. The \index{opposite!group}
\index{group!opposite~group}opposite group of a group $(G,\cdot)$ is
the group $(G,\cdot^\op)$ where $a\cdot^\op b=b\cdot a$ for any
$a,b\in G$.

The \index{center!of~a~group} \index{group!center}\textbf{center} of
$(G,\cdot)$ is the subgroup formed by the elements $a\in G$ such that
$a\cdot b=b\cdot a$ for any $a\in G$. The center is clearly a
commutative group.

Let $A$ be a subset of $G$, the group
\index{group!generator}\index{generator!of~a~group}\textbf{generated}
by $A$, denoted by $\langle A\rangle$, is the smallest subgroup of
$(G,\cdot)$ containing $A$. A finite group generated by a single
element is said to be
\index{cyclic!group}\index{group!cyclic}\textbf{cyclic}.

Let $(G,\cdot)$ be a group, $(H,\cdot)$ a subgroup and $g\in G$. The
subset $g\cdot H = \{g\cdot h | h \in H \}$ of $G$ is called a
\index{coset!left~coset}\textbf{left coset} of $H$, or simply
\index{coset}coset. A \index{coset!right~coset}\textbf{right coset},
denoted by $H\cdot g$, is a left coset for the opposite group; when
$G$ is abelian the two notions coincide. A subgroup $H$ is called
\index{subgroup!normal}normal if $g\cdot H=H\cdot g$ for any $g\in G$;
note that if $G$ is abelian, any subgroup is normal. Let $H$ be
normal, the cosets of $H$ form a group under the law $(g\cdot
H,g'\cdot H)\mapsto (g\cdot g')\cdot H$, this group is called the
\index{quotient!of~groups}\textbf{quotient} of $G$ by $H$ and is
denoted by $G/H$.

Let $S$ be a set and let $G$ be a group. A
\index{group~action!left~group~action}
\index{group~action}\textbf{(left) group action} of $G$ over $S$ is a
law $.:G\times S\ra S$ such that for any $g,g'\in G$ and $x\in S$,
\begin{itemize}
\item $g'.(g.x) = (g'\cdot g). x$,
\item $e. x = x$.
\end{itemize}
A \index{group~action!right~group~action}\textbf{right group action}
is a left group action for the opposite group.


\subsubsection{Rings, Fields}

A \index{ring}\textbf{ring} is a tuple $(R,+,\cdot)$ such that $(R,+)$
is an abelian group and $\cdot:R\times R\ra R$ is an internal
composition law satisfying associativity, existence of the identity
and \index{distributivity}\textbf{distributivity} over $+$
\[a \cdot (b + c) = (a\cdot b) + (a\cdot c) \quad\text{for any
  $a,b,c\in R$.}\] When $\cdot$ satisfies commutativity, the ring is
said to be \index{ring!commutative}\textbf{commutative}.  The law $+$
is called \emph{addition}, $\cdot$ is called \emph{multiplication},
the identity for $+$ is denoted by $0$ and the identity for $\cdot$ by
$1$.  A commutative ring such that $1\ne 0$ and where $\cdot$ also
satisfies the existence of the inverse, is called a
\index{field}\textbf{field}.

A \index{subring}\textbf{subring} of a ring $(R,+,\cdot)$ is a ring
$(S,\ast,\circ)$ such that $(S,\ast)$ is a subgroup of $(R,+)$ and
$\circ$ is the restriction of $\cdot$ to $S$.  The
\index{opposite!ring}\index{ring!opposite~ring}\textbf{opposite ring}
of a ring $(R,+,\cdot)$ is the ring $(R,+,\cdot^\op)$
where $a\cdot^\op b=b\cdot a$ for any $a,b\in R$.

The simplest example of ring is $\Z$, the set of integers; the
rational numbers $\Q$ are an example of field, it is the
\emph{smallest} field containing $\Z$ as a subring. The
\index{ring!trivial}\textbf{trivial ring} is the ring composed of one
unique element $r=0=1$ with the evident laws; note that by definition
this is not a field.

\subsubsection{Modules, ideals, vector spaces}

Given a ring $(R,+,\cdot)$ a \index{module!left~module}\textbf{left
  module}, or simply \textbf{module}, over $R$ is a tuple $(M, +_M,
\cdot_M)$ such that $(M,+_M)$ is an abelian group and $\cdot_M:R\times
M\ra M$ is an \emph{external law} such that for any $r,r'\in R$ and
$m,m'\in R$
\begin{itemize}
\item $(r + r')\cdot_M m = (r \cdot_M m) +_M (r'\cdot_M m)$,
\item $r\cdot_M(m +_M m') = (r\cdot_M m) +_M (r'\cdot_M m)$,
\item $r'\cdot_M(r\cdot_M m ) = (r'r)\cdot_M m$,
\item $1\cdot_M m = m$.
\end{itemize}
The law $+_M$ is called \emph{addition}, its identity is denoted by
$0$; the law $\cdot_M$ is called
\index{multiplication!scalar}\index{scalar~multiplication}\textbf{scalar}
or
\index{mutliplication!external}\index{external~multiplication}\textbf{external}
multiplication.

A \index{module!right~module}\textbf{right module} is a left module
for the opposite ring $R^\op$, a
\index{module!two-sided}\textbf{two-sided module} also called
\index{bimodule}\textbf{bimodule} is an object that is both a left and
a right module. When $R$ is commutative, the three notions coincide
and we simply speak of a \index{module}\textbf{module}.  $R$-module is
another way of saying ``module over $R$''. When $\K$ is a field, a
$\K$-module is called a \index{vector~space}\textbf{$\K$-vector
  space}.

A \index{module!submodule}\index{submodule!left~submodule}left
(\index{submoudle!right~submodule}right,
\index{submodule!two-sided}two-sided) \textbf{submodule} of a left
(right, two-sided) $R$-module $(M,+,\cdot)$ is a left (right, two-sided)
$R$-module $(N,\ast,\circ)$ such that $(N,\ast)$ is a subgroup of
$(M,+)$ and $\circ$ is the restriction of $\cdot$ to $R\times N$.

The module containing one unique element with the evident laws is
called the \index{module!zero~module}\textbf{zero module}; any
$R$-module contains a submodule that is isomorphic to the zero module.
Any abelian group $(G,+)$ can be given a $\Z$-module structure by the
law
\[n\cdot g = \underbrace{g + \cdots + g}_{n\text{ times}} \text{.}\]
Any ring $R$ is trivially a two-sided module over itself; a
\index{ideal!left~ideal} (\index{ideal!right~ideal}right,
\index{ideal!two-sided}two-sided) \textbf{ideal} of a ring $R$ is a
submodule of the left (right, two-sided) $R$-module $R$.  When $R$ is
commutative one simply speaks of an \index{ideal}ideal.  

Any ring contains at least two submodules: the zero module and itself;
these are called the \index{ideal!trivial}\textbf{trivial ideals}. The
only non-trivial ideals of $\Z$ are the $n\Z$ for any $n\ne0,1$. A
field has no non-trivial ideals.

The \index{direct~sum}\textbf{direct sum} $M\oplus N$ of two
$R$-modules $(M,+_M,\cdot_M)$ and $(N,+_N,\cdot_N)$ is the module
$(M\times N,+,\cdot)$ where the laws $+$ and $\cdot$ are defined
component-wise. This generalizes to sums of an arbitrary number of
modules: let $(M_i)_{i\in I}$ be a sequence of $R$-modules, the direct
sum $\bigoplus_{i\in I}M_i$ is the $R$-module whose elements are the
sequences $(m_1,m_2,\ldots)$ where $m_i\in M_i$ and $m_i=0$ for all
but a finite number of them; the laws are defined component-wise.
Although less commonly used, there also exists a notion of
\index{direct~product}\textbf{direct product}: given $(M_i)_{i\in I}$,
the direct product $\prod_{i\in I}M_i$ is the $R$-module whose
elements are the sequences $(m_1,m_2,\ldots)$ where $m_i\in M_i$ with
the laws defined component-wise. Clearly, the two definition coincide
when $I$ is a finite set.

When $R$ is seen as an $R$-module over itself, we denote by $R^n$ the
direct sum $\bigoplus_{0<i\le n}R$ and by $R^\infty$ the direct sum
$\bigoplus_{i>0}R$. An $R$-module that is isomorphic to the direct sum
$\bigoplus_{I}R$ for some $I$ is called a
\index{module!free}\textbf{free module}. A \index{basis}\textbf{basis}
of a module $M$ is a family $(m_i)_{i\in I}$ of elements of $M$ such
that any $m\in M$ can be written as 
\begin{equation}
  \label{eq:module-basis}
  m = \sum_{i\in I} r_i\circ m_i
  \quad\text{with $r_i\in R$}
\end{equation}
in an unique way. Clearly, if we note by $e_i$ the element of
$\bigoplus_IR$ that has $1$ in the $i$-th position and $0$ elsewhere,
the family $(e_i)_{i\in I}$ forms a basis; hence, any free module has
a basis and, conversely, any module that has a basis is free. One
important statement about bases of modules is the following.

\begin{proposition}
  Any two bases for a free module $M$ over a commutative ring $R$ have
  the same cardinality.
\end{proposition}

For this reason, when $M$ is a free module over a commutative ring $R$
we call \index{dimension} \index{module!free!dimension~of}
\index{vector~space!dimension~of}\textbf{dimension} the cardinality of
any of its bases. It is a well known result in linear algebra that any
vector space has a basis, hence any $\K$-vector space is free as a
$\K$-module.

Given a module $M$ and a submodule $N$, the quotient group $M/N$ can
be given a module structure by the law $r\cdot(m+N)=(r\cdot m)+N$, it
is then called the \index{quotient!of~modules}\textbf{quotient
  module}. When $R$ is a ring and $I$ a module, the quotient $R/I$ can
also be given a ring structure by the law $(r+I)\cdot(r'+I)=(r\cdot
r')+I$, it is then called the \index{quotient!of~ring}\textbf{quotient
  ring}.

In what follows we will follow the common practice and abuse the
notation in several ways:
\begin{itemize}
\item When different groups (rings, modules) are involved in a
  statement, we will often use the same symbols for all the group
  (ring, module) laws; the context will always make clear which group
  law is involved.
\item We will often use sentences like ``Let $G$ be a group (ring,
  module)'' without explicitly giving the symbols for the laws: the
  symbols $+$ and $\cdot$ are then implied. A
  \index{additive~notation}
  \index{group!noted~additively}\textbf{group noted additively} will
  be a group whose law is denoted by $+$; a
  \index{multiplicative~notation}
  \index{group!noted~multiplicatively}\textbf{group noted
    multiplicatively} will be a group whose law is denoted by $\cdot$.
\item Recall that any abelian group is a $\Z$-module. For a $g\in G$,
  the external multiplication by an element $n\in\Z$ will be written
  $n\cdot g$ or $ng$ in additive notation, $g^n$ in multiplicative
  notation.
\item The operator $\cdot$ will always have higher precedence than
  $+$, and it will often be omitted. Thus $ab+cd$ is just a compact
  notation for $(a\cdot b) + (c\cdot b)$.
\end{itemize}


\subsection{Arrows}
\label{sec:ring-fields:arrows}

Given two groups $G$ and $G'$, a
\index{morphism!of~groups}\textbf{morphism} from $G$ to $G'$,
sometimes also called a
\index{homomorphism|see{morphism}}\textbf{group homomorphism}, is a
mapping $f:G\ra G'$ that \emph{commutes} with the group law and
\emph{preserves} the identity, that is
\begin{itemize}
\item $f(a\cdot b) = f(a) \cdot f(b)$,
\item $f(e) = e$.
\end{itemize}

A \index{morphism!of~rings}\textbf{morphism of rings} is a group
morphism that commutes with multiplication and preserves $1$, a
\index{morphism!of~modules}\textbf{morphism of modules} is a group
morphism that commutes with the external multiplication.

The \index{kernel}\textbf{kernel} of a group morphism $f:G\ra G'$,
denoted by $\ker f$, is the subgroup of $G$ consisting of the elements
such that $f(g)=e$. The kernel of a ring (module) morphism is the
kernel of the underlying group morphism.

The \index{image}\textbf{image} of a group morphism $f:G\ra G'$,
denoted by $\im f$, is the subgroup of $G'$ consisting of the elements
$g'$ such that $f(g)=g'$ for some $g\in G$. The image of a ring
(module) morphism is the image of the underlying group morphism.

The set of morphisms from $A$ to $B$ is noted $\hom(A,B)$. If $A$ and
$B$ are (abelian) groups, then $\hom (A,B)$ is a (abelian) group by
the law $(f\cdot g)(a) = f(a)\cdot g(a)$. Similarly, if $A$ and $B$
are (commutative) rings, then $\hom(A,B)$ is a (commutative) ring. On
the other hand, it is not necessarily the case that if $A$ and $B$ are
$R$-modules, then $\hom(A,B)$ is an $R$-module; this is only true if
$R$ is a commutative ring.

A morphism $f:A\ra B$ is said to be
\index{morphism!injective}\textbf{injective} is its kernel is the zero
subgroup of $A$, \index{morhpism!surjective}\textbf{surjective} if its
image is the whole $B$, \index{morphism!bijective}\textbf{bijective}
if it is both. A bijective morphism is also called an
\index{isomorphism}\textbf{isomorphism}, two groups (rings, modules)
such that there is an isomorphism between them are said to be
\index{isomorphic}\textbf{isomorphic}, if $A$ and $B$ are isomorphic
we write $A\isom B$. A morphism $f:A\ra A$ is also called an
\index{endomorphism}\textbf{endomorphism}, a bijective endomorphism is
called an \index{automorphism}\textbf{automorphism}.

\begin{proposition}
  The kernel of a group morphism $f:A\ra B$ is a normal subgroup of
  $A$ and the image is a subgroup of $B$.  If $f$ is a ring morphism,
  $\ker f$ is an ideal and $\im f$ a subring. If $f$ is a module
  morphism, $\ker f$ and $\im f$ are both submodules.
  
  In each of these cases $\im f$ is isomorphic to $A/\ker f$ as a
  group/ring/module.
\end{proposition}

A convenient way of expressing properties of morphisms is to draw
\index{diagram}\textbf{diagrams} where objects (groups, rings, etc.)
are connected by arrows representing morphism. A diagram
\[\xymatrix{A\ar[r]^f\ar[dr]_h & B \ar[d]^g\\&C}\]
is said to be \index{diagram!commutative}\textbf{commutative} if
$g\circ f = h$. In general, we call \index{sequence}\textbf{sequence}
any sequence of consecutive arrows in a diagram. A diagram is
commutative if, whenever two sequences
\[\xymatrix{A\ar[r]^{f_1} & \cdots \ar[r]^{f_n} & B}\]
and
\[\xymatrix{A\ar[r]^{g_1} & \cdots \ar[r]^{g_n} & B}\]
connect the same two objects, $f_1\circ\cdots\circ
f_n=g_1\circ\cdots\circ g_m$.  A sequence
\[\xymatrix{\cdots \ar[r] & A_{i-1}\ar[r]^{f_{i-1}} & A_i \ar[r]^{f_i}
  & A_{i+1} \ar[r] & \cdots}\]
is said to be \index{sequence!exact}
\index{exact~sequence}\textbf{exact} if $\ker f_i = \im f_{i-1}$ for
any $i$.


\section{Linear algebra}
\label{sec:linear-algebra}

We apply concepts from classical linear algebra to free modules over
rings; we assume that the reader is familiar with matrices and the
related language. In this section $R$ will be a non necessarily
commutative ring and module will mean $R$-module.

\subsection{Matrices}
\label{sec:linear-algebra:matrices}

Consider the module $R^n$, if we represent elements of $R^n$ as row
vectors, any morphism $f:R^n\ra R^m$ can be represented by an $n\times
m$ matrix with entries in $R$, and the application of $f$ to $x\in
R^n$ is vector-matrix product\footnote{Many textbooks prefer to
  represent elements as column vectors and application as
  matrix-vector product. We take a different convention in order to be
  consistent with the fact that by ``module'' we mean ``left
  module''.}. More generally, a morphism $f:M\ra N$ of free modules
can be represented by a matrix whenever $M$ and $N$ have finite bases;
however the matrix --and even its dimensions-- depend on the choice of
the bases. When a morphism can be represented by a matrix, we
sometimes call it \index{linear!operator}\textbf{linear operator} or
\index{linear!map}\textbf{linear map}.

As we already pointed out, $\hom(M,N)$ is an abelian group, but not a
module. Thus sum of morphisms is well defined and coincides with the
usual sum of matrices, but the scalar (left) product of a matrix by an
element of $R$ does not coincide to any operation on morphisms.


\subsection{Duality}
\label{sec:linear-algebra:duality}

Let $R$ be a ring and $M$ a module over it. Consider the space
$\hom(M,R)$ of module homomorphism from $M$ to $R$, it has a natural
structure of $R^\op$-module by the assignment
\[(s^{\op}h)(r) = h(r)s \quad\text{for $r,s\in R, h\in\hom(M,R)$.}\]
Furthermore, if $M=R^n$, any $h\in\hom(R^n,R)$ is totally determined
by the images of the elements $e_i$ of the canonical basis, in fact
\[f(m) = f\left(\sum_im_ie_i\right) = \sum_im_if(e_i) \quad\text{for
  any $m\in R^n$.}\] This implies an $R^\op$-module isomorphism
$\hom(R^n,R)\simeq (R^{\op})^n$. $\hom(M,R)$ is called the \emph{dual
  space} of $M$ and is noted $\dual{M}$.

Consider now two $R$-modules $M$, $N$. Any homomorphism $f:M\ra N$
induces a map $\dual{f}:\dual{N}\ra\dual{M}$ by the assignment
\[\dual{f}(h) = h\circ f \quad\text{for any $h\in\hom(N,R)$.}\]
The map $\dual{f}$ is a homomorphism of $R^\op$-modules as it can be
seen by
\[\dual{f}(s^{\op}h)(r) = h(f(r))s = (s^{\op}(h\circ f))(r)
\quad\text{for $r,s\in R,h\in\hom(N,R)$.}\] It is trivial to verify
that $\dual{f}\circ\dual{g}=\dual{(g\circ f)}$ and this makes
$\dual{()}$ into a contravariant functor from $R$ to
$R^{\op}$. The functor $\dual{()}$ need not be full and
faithful as in the vector space case, but it is if we restrict to free
modules and the usual relationship $\dual{(\dual{M})}\simeq M$ holds
(although the isomorphism is not natural).

If, moreover, we restrict to the modules $R^n$ for finite $n$, the
functor $\dual{()}$ becomes a \emph{duality} of categories. Then we
can interpret everything in the simpler language of matrices, but care
must be taken since left module homomorphisms will act as
multiplication \emph{on the right}. We interpret elements of $R^n$ as
row vectors and elements of $\dual{(R^n)}$ as column vectors; a
mapping $f:R^m\ra R^n$ and its dual $\dual{f}$ both correspond to an
$m\times n$ matrix with entries in $R$: multiplication on the left by
a row vector corresponds to applying $f$ to an element of $R^m$,
multiplication on the right corresponds to applying $\dual{f}$ to an
element of $\dual{(R^n)}$. Multiplication of matrices corresponds to
composition of homomorphisms. Our correspondence may seem odd as one
is used to express linear applications as matrices that apply to
column vectors on the right; if this convention is preferred, taking
vectors and matrices with coefficients in $R^\op$ is equivalent to the
usual convention.


duality
matrices, trace, determinant, resultant

\section{Basic Galois theory}
\label{sec:basic-galois-theory}
Field extension, splitting field, Galois extensions, algebraic closure

roots of unity, cyclotomic polynomials

trace, norm

Artin-Schreier

finite fields


\section{Elliptic curves}
\label{sec:elliptic-curves}



%%% Local Variables: 
%%% mode:flyspell
%%% ispell-local-dictionary:"american"
%%% mode: TeX-PDF
%%% mode: reftex
%%% TeX-master: "../these"
%%% End: 


