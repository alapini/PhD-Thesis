Here we recall the basic concepts from abstract algebra that
constitute the background for all the chapters that follow. To the
reader interested in reading more about these topics, we recommend
\cite{lang} for general algebra, \cite{lidl+niederreiter:2} for finite
fields and \cite{silverman:elliptic,silverman:advanced} for elliptic
curves.

\section{Groups, Rings, Modules}
\label{sec:ring-fields}

\subsection{Objects}
\label{sec:ring-fields:objects}

\subsubsection{Groups}

A \index{group}\textbf{group} is a pair $(G,\cdot)$ such that $G$ is a
set and $\cdot:G\times G\ra G$ is an \emph{internal composition law}
satisfying:
\begin{itemize}
\item \index{associativity}\textbf{Associativity}: $(a\cdot b)\cdot c
  = a \cdot (b\cdot c)$ for any $a,b\in G$;
\item There is an element $e\in G$, called the
  \index{identity~element}\textbf{identity}, such that $a\cdot e =
  e\cdot a = a$ for any $a\in G$;
\item For any $a\in G$ there is an
  \index{inverse~element}\textbf{inverse element} $a^{-1}$ such that
  $a\cdot a^{-1} = a^{-1}\cdot a = e$.
\end{itemize}
If $\cdot$ also satisfies $a\cdot b=b\cdot a$
(\index{commutativity}\textbf{commutativity}), the group is said to be
\index{group!abelian}\textbf{abelian}. The group composed of one
single element with the obvious law is called the
\index{group!trivial}\textbf{trivial group}.

A \index{subgroup}\textbf{subgroup} of a group $(G,\cdot)$ is a group
$(H,\circ)$ such that $H\subset G$ and $\circ$ is the restriction of
$\cdot$ to $H$. Any group has two trivial subgroups: the trivial group
and itself. The \index{opposite!group}
\index{group!opposite~group}opposite group of a group $(G,\cdot)$ is
the group $(G,\cdot^\op)$ where $a\cdot^\op b=b\cdot a$ for any
$a,b\in G$.

The \index{center!of~a~group} \index{group!center}\textbf{center} of
$(G,\cdot)$ is the subgroup formed by the elements $a\in G$ such that
$a\cdot b=b\cdot a$ for any $a\in G$. The center is clearly a
commutative group.

Let $A$ be a subset of $G$, the group
\index{group!generator}\index{generator!of~a~group}\textbf{generated}
by $A$, denoted by $\langle A\rangle$, is the smallest subgroup of
$(G,\cdot)$ containing $A$. A finite group generated by a single
element is said to be
\index{cyclic!group}\index{group!cyclic}\textbf{cyclic}.

Let $(G,\cdot)$ be a group, $(H,\cdot)$ a subgroup and $g\in G$. The
subset $g\cdot H = \{g\cdot h | h \in H \}$ of $G$ is called a
\index{coset!left~coset}\textbf{left coset} of $H$, or simply
\index{coset}coset. A \index{coset!right~coset}\textbf{right coset},
denoted by $H\cdot g$, is a left coset for the opposite group; when
$G$ is abelian the two notions coincide. A subgroup $H$ is called
\index{subgroup!normal}normal if $g\cdot H=H\cdot g$ for any $g\in G$;
note that if $G$ is abelian, any subgroup is normal. Let $H$ be
normal, the cosets of $H$ form a group under the law $(g\cdot
H,g'\cdot H)\mapsto (g\cdot g')\cdot H$, this group is called the
\index{quotient!of~groups}\textbf{quotient} of $G$ by $H$ and is
denoted by $G/H$.

Let $S$ be a set and let $G$ be a group. A
\index{group~action!left~group~action}
\index{group~action}\textbf{(left) group action} of $G$ over $S$ is a
law $.:G\times S\ra S$ such that for any $g,g'\in G$ and $x\in S$,
\begin{itemize}
\item $g'.(g.x) = (g'\cdot g). x$,
\item $e. x = x$.
\end{itemize}
A \index{group~action!right~group~action}\textbf{right group action}
is a left group action for the opposite group.


\subsubsection{Rings, Fields}

A \index{ring}\textbf{ring} is a tuple $(R,+,\cdot)$ such that $(R,+)$
is an abelian group and $\cdot:R\times R\ra R$ is an internal
composition law satisfying associativity, existence of the identity
and \index{distributivity}\textbf{distributivity} over $+$
\[a \cdot (b + c) = (a\cdot b) + (a\cdot c) \quad\text{for any
  $a,b,c\in R$.}\] When $\cdot$ satisfies commutativity, the ring is
said to be \index{ring!commutative}\textbf{commutative}.  The law $+$
is called \emph{addition}, $\cdot$ is called \emph{multiplication},
the identity for $+$ is denoted by $0$ and the identity for $\cdot$ by
$1$.  A commutative ring such that $1\ne 0$ and where $\cdot$ also
satisfies the existence of the inverse, is called a
\index{field}\textbf{field}.

A \index{subring}\textbf{subring} of a ring $(R,+,\cdot)$ is a ring
$(S,\ast,\circ)$ such that $(S,\ast)$ is a subgroup of $(R,+)$ and
$\circ$ is the restriction of $\cdot$ to $S$.  The
\index{opposite!ring}\index{ring!opposite~ring}\textbf{opposite ring}
of a ring $(R,+,\cdot)$ is the ring $(R,+,\cdot^\op)$
where $a\cdot^\op b=b\cdot a$ for any $a,b\in R$.

The simplest example of ring is $\Z$, the set of integers; the
rational numbers $\Q$ are an example of field, it is the
\emph{smallest} field containing $\Z$ as a subring. The
\index{ring!trivial}\textbf{trivial ring} is the ring composed of one
unique element $r=0=1$ with the evident laws; note that by definition
this is not a field.

\subsubsection{Modules, ideals, vector spaces}

Given a ring $(R,+,\cdot)$ a \index{module!left~module}\textbf{left
  module}, or simply \textbf{module}, over $R$ is a tuple $(M, +_M,
\cdot_M)$ such that $(M,+_M)$ is an abelian group and $\cdot_M:R\times
M\ra M$ is an \emph{external law} such that for any $r,r'\in R$ and
$m,m'\in R$
\begin{itemize}
\item $(r + r')\cdot_M m = (r \cdot_M m) +_M (r'\cdot_M m)$,
\item $r\cdot_M(m +_M m') = (r\cdot_M m) +_M (r'\cdot_M m)$,
\item $r'\cdot_M(r\cdot_M m ) = (r'r)\cdot_M m$,
\item $1\cdot_M m = m$.
\end{itemize}
The law $+_M$ is called \emph{addition}, its identity is denoted by
$0$; the law $\cdot_M$ is called
\index{multiplication!scalar}\index{scalar~multiplication}\textbf{scalar}
or
\index{mutliplication!external}\index{external~multiplication}\textbf{external}
multiplication.

A \index{module!right~module}\textbf{right module} is a left module
for the opposite ring $R^\op$, a
\index{module!two-sided}\textbf{two-sided module} also called
\index{bimodule}\textbf{bimodule} is an object that is both a left and
a right module. When $R$ is commutative, the three notions coincide
and we simply speak of a \index{module}\textbf{module}.  $R$-module is
another way of saying ``module over $R$''. When $\K$ is a field, a
$\K$-module is called a \index{vector~space}\textbf{$\K$-vector
  space}.

A \index{module!submodule}\index{submodule!left~submodule}left
(\index{submoudle!right~submodule}right,
\index{submodule!two-sided}two-sided) \textbf{submodule} of a left
(right, two-sided) $R$-module $(M,+,\cdot)$ is a left (right, two-sided)
$R$-module $(N,\ast,\circ)$ such that $(N,\ast)$ is a subgroup of
$(M,+)$ and $\circ$ is the restriction of $\cdot$ to $R\times N$.

The module containing one unique element with the evident laws is
called the \index{module!zero~module}\textbf{zero module}; any
$R$-module contains a submodule that is isomorphic to the zero module.
Any abelian group $(G,+)$ can be given a $\Z$-module structure by the
law
\[n\cdot g = \underbrace{g + \cdots + g}_{n\text{ times}} \text{.}\]
Any ring $R$ is trivially a two-sided module over itself; a
\index{ideal!left~ideal} (\index{ideal!right~ideal}right,
\index{ideal!two-sided}two-sided) \textbf{ideal} of a ring $R$ is a
submodule of the left (right, two-sided) $R$-module $R$.  When $R$ is
commutative one simply speaks of an \index{ideal}ideal.  

Any ring contains at least two submodules: the zero module and itself;
these are called the \index{ideal!trivial}\textbf{trivial ideals}. The
only non-trivial ideals of $\Z$ are the $n\Z$ for any $n\ne0,1$. A
field has no non-trivial ideals.

The \index{direct~sum}\textbf{direct sum} $M\oplus N$ of two
$R$-modules $(M,+_M,\cdot_M)$ and $(N,+_N,\cdot_N)$ is the module
$(M\times N,+,\cdot)$ where the laws $+$ and $\cdot$ are defined
component-wise. This generalizes to sums of an arbitrary number of
modules: let $(M_i)_{i\in I}$ be a sequence of $R$-modules, the direct
sum $\bigoplus_{i\in I}M_i$ is the $R$-module whose elements are the
sequences $(m_1,m_2,\ldots)$ where $m_i\in M_i$ and $m_i=0$ for all
but a finite number of them; the laws are defined component-wise.
Although less commonly used, there also exists a notion of
\index{direct~product}\textbf{direct product}: given $(M_i)_{i\in I}$,
the direct product $\prod_{i\in I}M_i$ is the $R$-module whose
elements are the sequences $(m_1,m_2,\ldots)$ where $m_i\in M_i$ with
the laws defined component-wise. Clearly, the two definition coincide
when $I$ is a finite set.

When $R$ is seen as an $R$-module over itself, we denote by $R^n$ the
direct sum $\bigoplus_{0<i\le n}R$ and by $R^\infty$ the direct sum
$\bigoplus_{i>0}R$. An $R$-module that is isomorphic to the direct sum
$\bigoplus_{I}R$ for some $I$ is called a
\index{module!free}\textbf{free module}. A \index{basis}\textbf{basis}
of a module $M$ is a family $(m_i)_{i\in I}$ of elements of $M$ such
that any $m\in M$ can be written as 
\begin{equation}
  \label{eq:module-basis}
  m = \sum_{i\in I} r_i\circ m_i
  \quad\text{with $r_i\in R$}
\end{equation}
in an unique way. Clearly, if we denote by $e_i$ the element of
$\bigoplus_IR$ that has $1$ in the $i$-th position and $0$ elsewhere,
the family $(e_i)_{i\in I}$ forms a basis; hence, any free module has
a basis and, conversely, any module that has a basis is free. One
important statement about bases of modules is the following.

\begin{proposition}
  Any two bases for a free module $M$ over a commutative ring $R$ have
  the same cardinality.
\end{proposition}

For this reason, when $M$ is a free module over a commutative ring $R$
we call \index{dimension} \index{module!free!dimension~of}
\index{vector~space!dimension~of}\textbf{dimension} the cardinality of
any of its bases. It is a well known result in linear algebra that any
vector space has a basis, hence any $\K$-vector space is free as a
$\K$-module.

Given a module $M$ and a submodule $N$, the quotient group $M/N$ can
be given a module structure by the law $r\cdot(m+N)=(r\cdot m)+N$, it
is then called the \index{quotient!of~modules}\textbf{quotient
  module}. When $R$ is a ring and $I$ a module, the quotient $R/I$ can
also be given a ring structure by the law $(r+I)\cdot(r'+I)=(r\cdot
r')+I$, it is then called the \index{quotient!of~ring}\textbf{quotient
  ring}.

In what follows we will follow the common practice and abuse the
notation in several ways:
\begin{itemize}
\item When different groups (rings, modules) are involved in a
  statement, we will often use the same symbols for all the group
  (ring, module) laws; the context will always make clear which group
  law is involved.
\item We will often use sentences like ``Let $G$ be a group (ring,
  module)'' without explicitly giving the symbols for the laws: the
  symbols $+$ and $\cdot$ are then implied. A
  \index{additive~notation}
  \index{group!written~additively}\textbf{group written additively}
  will be a group whose law is denoted by $+$; a
  \index{multiplicative~notation}
  \index{group!written~multiplicatively}\textbf{group written
    multiplicatively} will be a group whose law is denoted by $\cdot$.
\item Recall that any abelian group is a $\Z$-module. For a $g\in G$,
  the external multiplication by an element $n\in\Z$ will be written
  $n\cdot g$ or $ng$ in additive notation, $g^n$ in multiplicative
  notation.
\item The operator $\cdot$ will always have higher precedence than
  $+$, and it will often be omitted. Thus $ab+cd$ is just a compact
  notation for $(a\cdot b) + (c\cdot b)$.
\item To avoid ambiguities, in particular when the $\cdot$ operator is
  omitted, the elements of the opposite group/ring will be denoted by
  $a^\op$ for any $a\in G$. Thus $a^\op b^\op=(ba)^\op$.
\end{itemize}


\subsection{Arrows}
\label{sec:ring-fields:arrows}

Given two groups $G$ and $G'$, a
\index{morphism!of~groups}\textbf{morphism} from $G$ to $G'$,
sometimes also called a
\index{homomorphism|see{morphism}}\textbf{group homomorphism}, is a
mapping $f:G\ra G'$ that \emph{commutes} with the group law and
\emph{preserves} the identity, that is
\begin{itemize}
\item $f(a\cdot b) = f(a) \cdot f(b)$,
\item $f(e) = e$.
\end{itemize}

A \index{morphism!of~rings}\textbf{morphism of rings} is a group
morphism that commutes with multiplication and preserves $1$, a
\index{morphism!of~modules}\textbf{morphism of modules} is a group
morphism that commutes with the external multiplication.

The \index{kernel}\textbf{kernel} of a group morphism $f:G\ra G'$,
denoted by $\ker f$, is the subgroup of $G$ consisting of the elements
such that $f(g)=e$. The kernel of a ring (module) morphism is the
kernel of the underlying group morphism.

The \index{image}\textbf{image} of a group morphism $f:G\ra G'$,
denoted by $\im f$, is the subgroup of $G'$ consisting of the elements
$g'$ such that $f(g)=g'$ for some $g\in G$. The image of a ring
(module) morphism is the image of the underlying group morphism.

A morphism $f:A\ra B$ is said to be
\index{morphism!injective}\textbf{injective} is its kernel is the zero
subgroup of $A$, \index{morhpism!surjective}\textbf{surjective} if its
image is the whole $B$, \index{morphism!bijective}\textbf{bijective}
if it is both. A bijective morphism is also called an
\index{isomorphism}\textbf{isomorphism}, two groups (rings, modules)
such that there is an isomorphism between them are said to be
\index{isomorphic}\textbf{isomorphic}, if $A$ and $B$ are isomorphic
we write $A\isom B$. A morphism $f:A\ra A$ is also called an
\index{endomorphism}\textbf{endomorphism}, a bijective endomorphism is
called an
\index{automorphism}\textbf{automorphism}. 

Sometimes, in the special case of $R$-module morphisms, we say
\index{linear!map}\textbf{$R$-linear map} instead of morphism, and
\index{linear!operator}\textbf{$R$-linear operator} on $M$ instead of
endomorphism of $M$. Linear maps $f:M\ra R$ are also called
\index{form!linear|see{linear}} \index{linear!form}\textbf{$R$-linear
  forms}.

\begin{proposition}
  The kernel of a group morphism $f:A\ra B$ is a normal subgroup of
  $A$ and the image is a subgroup of $B$.  If $f$ is a ring morphism,
  $\ker f$ is an ideal and $\im f$ a subring. If $f$ is a module
  morphism, $\ker f$ and $\im f$ are both submodules.
  
  In each of these cases $\im f$ is isomorphic to $A/\ker f$ as a
  group/ring/module.
\end{proposition}

\index{morphism!zero~morphism} \textbf{Zero morphisms} are those
morphisms $f:A\ra B$ such that $\ker f = A$.  An easy consequence of
the proposition is that, since a field has no non-trivial ideals, any
non-zero morphism of fields is injective.

The set of group/ring/module morphisms from $A$ to $B$ is denoted by
$\hom(A,B)$. If $A$ and $B$ are (abelian) groups, then $\hom (A,B)$ is
a (abelian) group by the law $(f\cdot g)(a) = f(a)\cdot g(a)$. If $A$
and $B$ are $R$-modules, the set of morphisms of modules may also be
written $\hom_R(A,B)$ if the ring is not clear from the context; if
$A$ and $B$ are right modules, the set of morphisms is conveniently
written as $\hom_{R^\op}(A,B)$. In general $\hom_R(A,B)$ and
$\hom_{R^\op}(A,B)$ are only abelian groups, but when $R$ is
commutative they are equal and they also have an $R$-module structure
by the law $(a\cdot f)(b) = a\cdot f(b)$.

If $A$ is a group/ring/module, $\hom(A,A)$ has a ring structure by the
law $f\cdot g=f\circ g$, where $\circ$ is composition of functions. It
is called the \index{ring!of~endomorphisms}
\index{endomorphism~ring}endomorphism ring of $A$ and is denoted by
$\End(A)$. When $A$ is a module over a commutative ring and we want to
stress the fact that $\End(A)$ has also a structure of $R$-module, we
write $\End_R(A)$.

Let $M$ be a left $R$-module, then $\hom_R(M,R)$ is a right $R$-module
by the law $(a^\op \cdot f)(b) = f(b)\cdot a$; similarly, if $M$ is a
right $R$-module, $\hom_{R^\op}(M,R)$ is a left $R$-module (a right
$R^\op$-module). Both are called the
\index{module!dual~module}\index{dual!of~a~module}\textbf{dual module}
of $M$ and are denoted by $\dual{M}$.

A convenient way of expressing properties of morphisms is to draw
\index{diagram}\textbf{diagrams} where objects (groups, rings, etc.)
are connected by arrows representing morphism. A diagram
\[\xymatrix{A\ar[r]^f\ar[dr]_h & B \ar[d]^g\\&C}\]
is said to be \index{diagram!commutative}\textbf{commutative} if
$g\circ f = h$. In general, we call \index{sequence}\textbf{sequence}
any sequence of consecutive arrows in a diagram. A diagram is
commutative if, whenever two sequences
\[\xymatrix{A\ar[r]^{f_1} & \cdots \ar[r]^{f_n} & B}\]
and
\[\xymatrix{A\ar[r]^{g_1} & \cdots \ar[r]^{g_n} & B}\]
connect the same two objects, $f_1\circ\cdots\circ
f_n=g_1\circ\cdots\circ g_m$.  A sequence
\[\xymatrix{\cdots \ar[r] & A_{i-1}\ar[r]^{f_{i-1}} & A_i \ar[r]^{f_i}
  & A_{i+1} \ar[r] & \cdots}\]
is said to be \index{sequence!exact}
\index{exact~sequence}\textbf{exact} if $\ker f_i = \im f_{i-1}$ for
any $i$.


\section{Linear algebra}
\label{sec:linear-algebra}

We apply concepts from classical linear algebra to free modules over
rings; we assume that the reader is familiar with matrices and the
related language. In this section $R$ will be a non necessarily
commutative ring and module will mean $R$-module.

\subsection{Bra-ket notation}
\label{sec:linear-algebra:bra-ket}

It will be convenient to (ab)use Dirac's
\index{bra-ket~notation}bra-ket notation to represent elements of
modules. If $(M,+,\cdot)$ is a left $R$-module and $x\in (M,+)$ is an
element of its underlying group, by $\ket{x}_R$ we mean the element
obtained by lifting $x$ in $(M,+,\cdot)$. We call $\ket{x}$ a
\index{ket}\textbf{ket} and read it as ``ket x''.

The external multiplication by an element $a\in R$ will be written
$a\ket{x}_R$; if $f:M\ra N$ is a left module morphism, we write
$f\ket{x}_R$ for $f(\ket{x}_R)$. By a slight abuse of notation we may
write $\ket{a x}_R$ and $\ket{f(x)}_R$ for $a\ket{x}_R$ and
$f\ket{x}_R$ respectively. When $R$ is clear from the context, a ket
can be simply written as $\ket{x}$.

In a symmetric way, elements of right $R$-modules will be written
${}_R\bra{x}$, which we call a \index{bra}\textbf{bra} and read as
``bra x''. External multiplication will be written as ${}_R\bra{x}a$
and application of a right module morphism as ${}_R\bra{x}f$.

Let $M$ be a right module and $N$ a left module. A
\index{form!bilinear|see{bilinear~form}}
\index{bilinear~form}\textbf{bilinear form} on $M\times N$ is a map
$f:M\times N\ra R$ such that for any $x\in M$, the map
\[\ket{y}\mapsto f(x, y)\]
is a left module morphism, and for any $y\in N$, the map
\[\bra{x}\mapsto f(x, y)\]
is a right module morphism. If $f$ is a bilinear form, we write
$\braket{x}{y}_f$ for $f(x,y)$, or simply $\braket{x}{y}$
when $f$ is clear from the context. Note that textbooks usually define
bilinear forms only when $R$ is commutative, in our more general
setting some common properties of bilinear forms fail to hold,
for example $\braket{xa}{y}\ne\braket{x}{ay}$.

Any bilinear form $f$ gives rise to a morphism $ \phi_f : M \ra
\dual{N}$ of right modules where $\bra{x}\phi_f$ is the linear form $y
\mapsto \braket{x}{y}$. Similarly, $f$ gives rise to a morphism
$\phi^f:N\ra\dual{M}$ of left modules. The maps $f\mapsto\phi_f$,
$f\mapsto\phi^f$ and their obvious inverses induce group isomorphisms
between $\hom(M,\dual{N})$, $\hom(N,\dual{M})$ and the group of
bilinear forms on $M\times N$.

A bilinear form $f$ is said to be
\index{bilinear~form!degenerate}\textbf{non-degenerate} if
$\braket{x}{y}=0$ implies $x=0$ and $y=0$. It is said to
be \index{bilinear~form!singular}\textbf{non-singular} if $\phi_f$ and
$\phi^f$ are module isomorphisms; a non-singular form is necessarily
non-degenerate.


\subsection{Matrices}
\label{sec:linear-algebra:matrices}
Let $M=M_1\oplus\cdots\oplus M_n$ and $N=N_1\oplus\cdots\oplus N_m$,
let $\iota_i$ be the injections $M_i\ra M$ and let $\pi_j$ be the
projections $N\ra N_i$, then a linear map $f:M\ra N$ is uniquely
determined by the maps $\pi_j\circ f\circ\iota_i$. If we consider
$m\times n$ matrices whose $(j,i)$-th coefficient is in
$\hom(M_i,N_j)$, then we verify that there is a group isomorphism
between $\hom(M,N)$ and this group of matrices. Furthermore, let
$f:M\ra N$ and $g:N\ra O$ and let $M_f$ and $M_g$ be the matrices that
are associated respectively, then the matrix associated to $g\circ f$
is $M_gM_f$, where the product of two entries is defined as
composition of morphisms. This induces a ring isomorphism between
$\End(M)$ and the ring of square matrices with entries in
$\hom(M_i,M_j)$.

Consider $R$ as an $R$-module over itself, a linear map from $R$ to
itself is uniquely determined by the image of $1$, hence $\End(R)\isom
R^\op$. As a consequence, there is a group isomorphism
$\hom(R^n,R^m)\isom\Mat_{m\times n}(R^\op)$ and matrix multiplication
is equivalent to composition as above.


\subsection{Duality}
\label{sec:linear-algebra:duality}
In this section we fix a non-singular bilinear form $f$ on $M\times
N$. Let $g\in\End_{R^\op}(M)$, then the map
\[(x,y) \mapsto \braket{g(x)}{y}_f\] is a bilinear
form. On the other hand, let $h$ be a bilinear form on $M\times N$,
for any $x\in M$ the map $h_x : \ket{y} \mapsto \braket{x}{y}_h$
is a linear form on $N$, thus $h_x\in\dual{N}$. From the
non-singularity of $f$ we deduce that there is an unique element
$x'\in M$ such that $\braket{x'}{y}_f=\braket{x}{y}_h$ and it is
clear that the map $\bra{x}\mapsto \bra{x'}$ is an endomorphism of
$M$. It is evident that the two maps are each other's inverse, thus we
have a group isomorphism between $\End(M)$ and the group of bilinear
forms. An analogous argument shows that $\End(N)$ is isomorphic to the
group of bilinear forms and ultimately $\End(M)\isom\End(N)$.

A consequence of this is that for any linear operator $g\in\End_{R^\op}(M)$
there is an operator $\dual{g}\in\End_R(N)$ such that
\[\braket{g(x)}{y}_f = \braket{x}{\dual{g}(y)}_f\]
for any $x\in M$ and $y\in N$. We define similarly $\dual{h}$ when
$h\in\End_R(N)$, obviously $\dual{(\dual{g})} = g$. The operator
$\dual{g}$ is called the \index{dual!operator}\textbf{dual} of $g$
with respect to $f$. In general, whenever it is clear from the context
that $g$ belongs to $\End(M)$ (or to $\End(N)$), we simply write
\[\braketop{x}{g}{y} = \braket{g(x)}{y} = \braket{x}{\dual{g}(y)}\text{.}\]

More generally, let 




Let $R$ be a ring and $M$ a module over it. Consider the space
$\hom(M,R)$ of module homomorphism from $M$ to $R$, it has a natural
structure of $R^\op$-module by the assignment
\[(s^{\op}h)(r) = h(r)s \quad\text{for $r,s\in R, h\in\hom(M,R)$.}\]
Furthermore, if $M=R^n$, any $h\in\hom(R^n,R)$ is totally determined
by the images of the elements $e_i$ of the canonical basis, in fact
\[f(m) = f\left(\sum_im_ie_i\right) = \sum_im_if(e_i) \quad\text{for
  any $m\in R^n$.}\] This implies an $R^\op$-module isomorphism
$\hom(R^n,R)\simeq (R^{\op})^n$. $\hom(M,R)$ is called the \emph{dual
  space} of $M$ and is noted $\dual{M}$.

Consider now two $R$-modules $M$, $N$. Any homomorphism $f:M\ra N$
induces a map $\dual{f}:\dual{N}\ra\dual{M}$ by the assignment
\[\dual{f}(h) = h\circ f \quad\text{for any $h\in\hom(N,R)$.}\]
The map $\dual{f}$ is a homomorphism of $R^\op$-modules as it can be
seen by
\[\dual{f}(s^{\op}h)(r) = h(f(r))s = (s^{\op}(h\circ f))(r)
\quad\text{for $r,s\in R,h\in\hom(N,R)$.}\] It is trivial to verify
that $\dual{f}\circ\dual{g}=\dual{(g\circ f)}$ and this makes
$\dual{()}$ into a contravariant functor from $R$ to
$R^{\op}$. The functor $\dual{()}$ need not be full and
faithful as in the vector space case, but it is if we restrict to free
modules and the usual relationship $\dual{(\dual{M})}\simeq M$ holds
(although the isomorphism is not natural).

If, moreover, we restrict to the modules $R^n$ for finite $n$, the
functor $\dual{()}$ becomes a \emph{duality} of categories. Then we
can interpret everything in the simpler language of matrices, but care
must be taken since left module homomorphisms will act as
multiplication \emph{on the right}. We interpret elements of $R^n$ as
row vectors and elements of $\dual{(R^n)}$ as column vectors; a
mapping $f:R^m\ra R^n$ and its dual $\dual{f}$ both correspond to an
$m\times n$ matrix with entries in $R$: multiplication on the left by
a row vector corresponds to applying $f$ to an element of $R^m$,
multiplication on the right corresponds to applying $\dual{f}$ to an
element of $\dual{(R^n)}$. Multiplication of matrices corresponds to
composition of homomorphisms. Our correspondence may seem odd as one
is used to express linear applications as matrices that apply to
column vectors on the right; if this convention is preferred, taking
vectors and matrices with coefficients in $R^\op$ is equivalent to the
usual convention.


duality
matrices, trace, determinant, resultant

\section{Basic Galois theory}
\label{sec:basic-galois-theory}
Field extension, splitting field, Galois extensions, algebraic closure

roots of unity, cyclotomic polynomials

trace, norm

Artin-Schreier

finite fields


\section{Elliptic curves}
\label{sec:elliptic-curves}



%%% Local Variables: 
%%% mode:flyspell
%%% ispell-local-dictionary:"american"
%%% mode: TeX-PDF
%%% mode: reftex
%%% TeX-master: "../these"
%%% End: 


