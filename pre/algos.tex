%% these.tex
%% Copyright 2010 Luca De Feo
%% All rights reserved


\section{Asymptotic complexity}
\label{sec:asympt-compl}
\index{complexity~notation} We shall measure the complexity of the
algorithms that appear in this document using the classical
\index{big-Oh|see{complexity~notation}} \symb[complex]{$O$}{big-Oh
  complexity notation}$O$ (big-Oh) notation. Given two functions
$f,g:\N\ra\N$, by $f\in O(g)$ we mean that there are an $x_0$ and a
constant $M$ such that
\begin{equation}
  \label{eq:239}
  f(x)<Mg(x) \quad\text{for any $x>x_0$.}  
\end{equation}
Similarly, we shall use the
\index{big-Omega|see{complexity~notation}}\symb[complex]{$\Omega$}{big-Omega
  complexity notation}$\Omega$ and
\index{big-Theta|see{complexity~notation}}\symb[complex]{$\Theta$}{big-Theta
  complexity notation}$\Theta$ notations to state lower bounds and
tight bounds: the definition of $f\in\Omega(g)$ is like
Eq.~\eqref{eq:239}, but with the inequality turned the other way. The
definition of $\Theta$ is $f\in\Theta(g)$ if and only if $f\in O(g)$
and $f\in\Omega(g)$.

We shall also make use of the notation \symb[complex]{$\tildO$}{soft-Oh
  complexity notation}$\tildO_x$
(\index{soft~Oh|see{complexity~notation}}soft-Oh of $x$) that forgets
polylogarithmic factors in the variable $x$, thus $O(xy\log x \log
y)\subset\tildO_x(xy\log y)\subset\tildO_{x,y}(xy)$. We simply write
$\tildO$ when the variables are clear from the context.

Many algorithms below rely on fast multiplication; thus, we let
$\Mult_R : \N \rightarrow \N$ be a
\index{multiplication~function}\emph{multiplication
  function}\symb[complex]{$\Mult(n)$}{Multiplication function}, such
that polynomials in $R[X]$ of degree less than $n$ can be multiplied
in $\Mult_R(n)$ arithmetic operations. We drop the index $R$ when the
ring is clear from the context. To simplify expressions,
following~\cite[$\S$8.3]{vzGG}, we shall assume that $\Mult(n)$ is
\begin{itemize}
\item superlinear: $\Mult(n)/n \ge \Mult(m)/m$ if $n\ge m$,
\item at most quadratic: $\Mult(mn)\le m^2\Mult(n)$.
\end{itemize}
We shall see soon that these assumptions are reasonable ones.

The cost of \index{modular~composition}\emph{modular composition}
--that is computing $f\circ g \bmod h$, for $f,g,h\in R[X]$ of degrees
at most $n$, and $h$ monic-- will be written
$\ModComp(n)$\symb[complex]{$\ModComp(n)$}{Modular composition
  function}. We shall make the assumption that
$\ModComp(n)\ge\Mult(n)$ for any $n$; the next section will review
algorithms for modular composition.


\section{Fundamental algorithms}
\label{sec:fund-algor}
In this section we review some fundamental algorithms that we will
repeatedly use in the rest of the document. Most of the algorithms we
present are taken from~\cite{vzGG}; another source of inspiration
is~\cite{poly-formel}.

\subsection{Polynomial multiplication}
\label{sec:polyn-mult}
Multiplication of polynomials with coefficients in a ring is a
fundamental underpinning to which most of the algorithms in computer
algebra reduce.

In the previous section we introduced the notation $\Mult(n)$ to
denote the number of operations in $R$ required to multiply two
polynomials of degree at most $n$ in $R[X]$.  Using the school-book
algorithm, we have $\Mult(n) \in O(n^2)$. The first major step forward
in the complexity of multiplication was done by \index{Karatsuba
  multiplication}Karatsuba~\cite{karatsuba}. He observed that using
the formula
\begin{gather*}
  f = f_1X^n + f_2\text{,}\qquad g = g_1X^n + g2\text{,}\\
  fg = f_1g_1X^{2n} + \bigl((f_1+f_2)(g_1+g_2)-f_1g_1-f_2g_2\bigr)X^n + f_2g_2
  \text{,}
\end{gather*}
multiplication can be computed recursively using only $3$ recursive
calls.  It follows that $\Mult(n)\in O(n^{\log_23})$.

When the base ring $R$ is a field containing a primitive $n$-th root
of unit $\omega$, polynomials can be multiplied by evaluating at the
powers of $\omega$, multiplying each evaluation, and interpolating
back. The map that sends a polynomial of degree $n$ over its
evaluations at the $n$-th roots of unit is called
\index{discrete~Fourier~transform}\emph{discrete Fourier transform},
there are many algorithms of complexity $O(n\log n)$ to compute it,
they all go under the generic name of
\index{FFT|see{fast Fourier transform}}\index{fast~Fourier~transform}\emph{fast Fourier transform}
(FFT).

\pdfmcone{More bibliography around Schönage and Strassen.}  Thus,
multiplication in certain fields can be carried out in $O(n\log n)$
operations. Schönhage and Strassen's method~\cite{schonage+strassen},
along with its generalizations~\cite{schonhage77,cantor+kaltofen91},
adjoins enough roots of unit to any ring $R$ by taking an extension of
it; this yields an algorithm of complexity $O(n\log n\loglog n)$ to
multiply polynomials of degree $n$ in $R[X]$.

\subsection{Formal power series}
\label{sec:formal-power-series}
We denote by $R[[X]]$ the ring of
\index{formal~power~series}\emph{formal power series} on $R$. Its
elements are the sequences $(f_i)_{i>0}$ of elements of $R$, they are
denoted by
\begin{equation}
  \label{eq:197}
  f(X) = \sum_{i\ge0}f_iX^i
  \text{.}
\end{equation}
Multiplication and evaluation are defined in the obvious way. An
element $f\in R[[X]]$ is invertible if and only if $f_0$ is an unit
of $R$.

Since formal power series are infinite objects, to be used in a
discrete algorithm they must be approximated. We denote by $f\bmod
X^n$ the polynomial
\begin{equation}
  \label{eq:198}
  f\bmod X^n = \sum_{0\le i < n}f_iX^i
  \text{.}
\end{equation}
We write $f = g + O(X^n)$, where $g$ is a polynomial or a power
series, whenever
\[f\bmod X^n=g\bmod X^n\text{,}\] and we say that $g$ approximates $f$
to the precision $n$.

Using polynomial multiplication, the product of two series known up to
precision $n$ can be computed in $\Mult(n)$ operations.

\paragraph{Derivative, integral}
\label{sec:derivative-integral}
\pdfmcone{Clearer statement of when we want R to contain Q.}
We define the \index{formal~power~series!derivative}derivative of a
power series as
\begin{equation}
  f'(X) = \sum_{i\ge0}(i+1)f_{i+1}X^i\text{;}
\end{equation}
if $R$ contains $\Q$, we also define the
\index{formal~power~series!integral}integral as
\begin{equation}
  \label{eq:200}
  \int f(X) = \sum_{i\ge0}\frac{f_i}{i+1}X^{i+1}\text{.}
\end{equation}
Derivatives and integrals up to precision $n$ can be computed in
$O(n)$ operations by their definition.

\paragraph{Logarithm, exponential}
\label{sec:logarithm}
In what follows, we suppose that $R$ contains $\Q$. The
\index{formal~power~series!logarithm}logarithm of a power series $f$
such that $f(0)=1$ is defined as
\begin{equation}
  \label{eq:194}
  \log f = \int\frac{f'}{f}
  \text{.}
\end{equation}
The \index{formal~power~series!exponential}exponential of a power
series $f$ such that $f(0)=0$, is defined as
\begin{equation}
  \label{eq:195}
  \exp(f) = 1 + f/1! + f^2/2! + \cdots
\end{equation}

\paragraph{First order linear differential equations}
\label{sec:first-order-linear}
All the usual identities involving multiplication, derivatives,
integrals, logarithms and exponentials are verified on power
series. An immediate consequence of this is a formula to solve first
order linear differential equations due to Brent and
Kung~\cite{brent+kung}.

Let $f,g\in
R[[X]]$, the equation
\begin{equation}
  \label{eq:208}
  y' = f(X)y + g(X)
\end{equation}
with initial condition $y(0)=a$ has solution
\begin{equation}
  \label{eq:209}
  y(X) =  \frac{1}{j(X)}\left( a + \int g(X)j(X)\right)
  \text{,}
\end{equation}
where $j = \exp(-\int f)$; the verification is immediate.

In the next subsection we shall see that multiplicative inverses,
logarithms, exponentials and powers up to precision $n$ can all be
computed in $O(\Mult(n))$ operations, thus formula~\eqref{eq:209} can
also be applied at the same cost.


\begin{nota}
  If $R$ does not contain $\Q$, but has characteristic $0$, it is easy
  to use the previous definitions by working in
  $R[2^{-1},3^{-1},\ldots]$ and taking the result back in $R$ when
  needed. In characteristic different from $0$, these definition do
  not make sense anymore because Eq.~\eqref{eq:200} introduces a
  division by $0$. However, when $2,3,\ldots,n$ are invertible in $R$,
  we can still do computations on power series truncated to the order
  $n$.
\end{nota}


\subsection{Newton's iteration}
\label{sec:newtons-iteration}
Let $\Phi:\R\ra\R$ be a $C^1$ function,
\index{Newton's~iteration}Newton's iteration is a classical method to
approximate a root $x$ of $\Phi$. Start from an approximation $x_0$,
and \emph{linearize} $\Phi$ to compute
\begin{equation}
  \label{eq:192}
  x_1 = x_0 - \frac{\Phi(x_0)}{\Phi'(x_0)}
  \text{,}
\end{equation}
then iterate this step until the desired precision is obtained. When
$x_0$ is taken close enough to a root, and when the derivative at this
root is non-zero, Newton's iteration converges \emph{quadratically} to
the solution, meaning that at each iteration the distance to the
solution is squared.

In computer algebra, Newton's iteration is applied to operators
$\Phi:R[[X]]\ra R[[X]]$ on formal power series; in this context,
\emph{quadratic} convergence means that the number of correct terms is
doubled at each iteration. Many fast algorithms for some fundamental
operations on power series and polynomials are obtained by this
method, here we summarize the most important ones.

\paragraph{Inversion}
If $f\in R[[X]]$ is invertible, the operator $\Phi(y) = 1/y - f$
applied to $y_0=1/f(0)$ converges quadratically to the inverse of
$f$. Since the iteration associated to $\Phi$ is
\begin{equation}
  \label{eq:193}
  y_{i+1} = y_i(2 - y_if)
  \text{,}
\end{equation}
the cost of inverting a power series is $O(\Mult(n))$. From
Eq.~\eqref{eq:194} we deduce that computing the logarithm of a power
series has the same cost.

Another important consequence of this algorithm is that the Euclidean
division of polynomials of degree at most $n$ can also be performed in
$O(\Mult(n))$ operations.

\paragraph{Exponential}
If $f$ is such that $f(0)=0$, we compute its exponential using the
operator $\Phi(y)=f-\log y$, which gives the iteration
\begin{equation}
  \label{eq:196}
  y_{i+1} = y_i(1 + f - \log y)
  \text{.}
\end{equation}
Thus, the cost of computing an exponential is $O(\Mult(n))$ too. Using
the formula
\begin{equation}
  \label{eq:201}
  f^\alpha = \exp(\alpha\log f)
  \text{,}
\end{equation}
we deduce that, in characteristic $0$, computing arbitrary rational
powers of power series costs $O(\Mult(n))$ too.


\subsection{Modular composition}
\label{sec:modular-composition}
Given polynomials $f,g,h\in R[X]$ of degree at most $n$ with $h$
monic, the \index{modular~composition}\emph{modular composition}
requires to compute
\begin{equation}
  \label{eq:190}
  f(g(X)) \mod h(X)
  \text{;}
\end{equation}
the special case where $h=X^n$ permits us to compute the
\index{formal~power~series!composition}\emph{composition of power
  series} truncated to the order $n$.

Modular composition is a fundamental algorithm with lots of
applications, the most relevant being polynomial
factorization~\cite{vzgathen+shoup92,kaltofen+shoup98} and computation
of minimal polynomials (see Remark~\ref{rk:shoups-algorithm-1}).

Since many algorithms in this document make use of modular
composition, we introduced the notation $\ModComp(n)$ for its
complexity. A naive algorithm implies $\ModComp(n)\in
O(n\Mult(n))$. The first improvement to this bound was given by Brent
and Kung in~\cite{brent+kung}: they devised a baby step-giant step
algorithm of complexity $O\left(\sqrt{n}\Mult(n) +
  n^{(\omega+1)/2}\right)$; in the same paper they also gave an
algorithm of complexity $O\left(\sqrt{n\log n}\Mult(n)\right)$ for
composition of power series. Bernstein~\cite{bernstein98} found the
bound $O(\Mult(n)\log n)$ for the composition of power series in case
the characteristic of the base ring is small, however for a long time
Brent and Kung's algorithm and its
variants~\cite{huang+pan98,kaltofen+shoup98} have stood as the only
generic algorithm for modular composition. A major breakthrough has
been recently achieved by Kedlaya and
Umans~\cite{umans:08,kedlaya+umans08}, who give an algorithm for
modular composition over a finite field $\F_q$ of \emph{binary}
complexity $n^{1+o(1)}\log^{1+o(1)}q$, using a reduction to
multivariate multipoint evaluation.


\paragraph{Computing iterated Frobenius and pseudotrace}
\label{sec:comp-frob-trace}
\index{iterated~Frobenius}\index{Frobenius~automorphism}
Fast modular composition can be used to compute Frobenius
automorphisms and \emph{pseudotraces} in finite fields. This algorithm
is due to von zur Gathen and Shoup~\cite{vzgathen+shoup92}, who
applied it to polynomial factorization. We will repeatedly use it in
Chapters~\ref{cha:artin-schr-towers} and~\ref{cha:algor-small-char}.

Consider the field extension $\F_{q^d}/\F_q$, its Galois group is
generated by the Frobenius automorphism
\begin{equation}
  \label{eq:199}
  \begin{aligned}
  \frob_q : \F_{q^d}&\ra\F_{q^d}\text{,}\\
  x &\mapsto x^{q}\text{.}
  \end{aligned}
\end{equation}
For any $n<d$, we also define the $n$-th
\index{pseudotrace}\emph{pseudotrace}\footnote{In~\cite{vzgathen+shoup92},
  this map goes under the name of \index{trace~map}\emph{trace map}.}
as
\begin{equation}
  \label{eq:202}
  \begin{aligned}
    \PTr_n : \F_{q^d}&\ra\F_{q^d}\text{,}\\
    x&\mapsto\sum_{i=0}^{n-1} x^{q^i}\text{.}
  \end{aligned}
\end{equation}
Notice that, when $n=d$, the pseudotrace coincides with the trace
$\Tr_{\F_{q^d}/\F_q}$; in this case one can use much faster
algorithms.


\begin{algorithm}
  \caption{Iterated Frobenius}
  \label{alg:itfrob}
  \begin{algorithmic}[1]
    \REQUIRE $0<i<d$, $a\in\F_q[X]/f(x)$, $\Phi_1(X) = X^q\bmod f(x)$.
    \ENSURE $\frob_q^i(a)$.
    \STATE let $i=\sum b_j2^j$ be the binary expansion of $i$;
    \STATE $k \la 1$;
    \FOR {$j=\lfloor\log_2 i\rfloor -1$ \TO $0$}
    \IF{$b_j=0$}
    \STATE $\Phi_{2k} \la \Phi_k\circ\Phi_k \bmod f$;
    \STATE $k \la 2k$;
    \ELSE
    \STATE $\Phi_{2k} \la \Phi_k\circ\Phi_k \bmod f$;
    \STATE $\Phi_{2k+1} \la \Phi_{2k}\circ\Phi_1 \bmod f$;
    \STATE $k \la 2k +1$;
    \ENDIF
    \ENDFOR
    \STATE return $a\circ\Phi_i \bmod f$.
  \end{algorithmic}
\end{algorithm}

We suppose that elements of $\F_{q^d}$ are represented as residue
classes in $\F_q[X]/f(X)$ for some irreducible polynomial $f$, then
the Frobenius automorphism can be computed with $O(\log q)$
multiplications in $\F_{q^d}$ plus one modular composition as
\begin{align}
  \label{eq:203}
  \Phi_1(X) &= X^q \bmod f(X)\text{,}\\
  \frob_q(a) &= X^q\circ a\bmod f = a\circ X^q\bmod f = a\circ \Phi_1\bmod f
  \text{.}
\end{align}


Iterating $i$ times $\frob_q$ can be done with only $O(\log i)$
modular compositions via square-and-multiply as shown in
Algorithm~\ref{alg:itfrob}.

\ifafourps\enlargethispage{\baselineskip}\fi

Thus the cost of computing the $i$-th iterated Frobenius is
\begin{equation}
  \label{eq:204}
  O(\ModComp(d)\log i)
\end{equation}
operations in $\F_q$ plus a precomputation costing $O(\Mult(d)\log
q)$.

\begin{algorithm}
  \caption{Pseudotrace}
  \label{alg:pseudotrace-vzgs}
  \begin{algorithmic}[1]
    \REQUIRE $0<i<d$, $a\in\F_q[X]/f(x)$, $\Phi_1(X) = X^q\bmod f(x)$.
    \ENSURE $\PTr_n(a)$.
    \STATE let $n=\sum b_j2^j$ be the binary expansion of $n$;
    \STATE $\Theta_o \la 0$, $\Theta_1 \la a\circ\Phi_{1}$;
    \STATE $k=b_0$;
    \FOR{$j=1$ \TO $\lfloor\log_2n\rfloor$}
    \STATE $\Phi_{2^j} \la \Phi_{2^{j-1}}\circ\Phi_{2^{j-1}} \bmod f$;
    \STATE $\Theta_{2^j} \la \Theta_{2^{j-1}} + \Theta_{2^{j-1}}\circ\Phi_{2^{j-1}} \bmod f$;
    \IF{$b_j=1$}
    \STATE $\Theta_{2^j+k} \la \Theta_{2^j} + \Theta_{k}\circ\Phi_{2^j} \bmod f$;
    \STATE $k \la 2^j + k$;
    \ENDIF
    \ENDFOR
    \STATE return $\Theta_n$.
  \end{algorithmic}
\end{algorithm}

In Algorithm~\ref{alg:pseudotrace-vzgs} we apply the same idea to
compute the $n$-th pseudotrace in $O(\ModComp(d)\log n)$ operations;
note that we use a dynamic programming technique to keep the
complexity into this bound. The key equation is
\begin{equation}
  \label{eq:205}
  \PTr_{n+m}(a) = \PTr_{n}(a)+\frob_q^n(\PTr_{m}(a))
  \text{.}
\end{equation}


\subsection{Interpolation and Chinese remainder algorithm}
\label{sec:chin-rema-algor}
Let $\K$ be a field, and let $x_1,\ldots,x_n\in\K$ be distinct
points. Let $f$ be the polynomial that vanishes on $x_1,\ldots,x_n$,
by the \titleref{th:chinese-remainder}
we know that
\begin{equation}
  \label{eq:221}
  \K[X]/f(X) \isom\prod_i \K[X]/(X-x_i)
  \text{.}
\end{equation}
If $a$ is an element on the left side of the isomorphism (i.e.\ a
polynomial modulo $f$), moving it to the right is evaluation at the
points $x_1,\ldots,x_n$. The inverse operation is interpolation. 

We now give two algorithms to perform this change of representation
efficiently. We suppose for simplicity that $n=2^k$. The first step is
to construct the \index{subproduct~tree}\emph{subproduct tree}.

\begin{algorithm}
  \caption{\label{alg:subprod}Subproduct tree}
  \begin{algorithmic}[1]
    \REQUIRE $n=2^k$, $x_1,\ldots,x_n\in\K$.
    \ENSURE The \emph{subproduct tree}.
    \STATE let $f_i^{(k)}=(X-x_i)$ for $0<i\le2^k$;
    \FOR {$j=k-1$ \TO $0$}
    \FORALL {$i\in[1,\ldots,2^j]$}
    \STATE $f^{(j)}_i \la f^{(j+1)}_if^{(j+1)}_{2i}$.
    \ENDFOR
    \ENDFOR
  \end{algorithmic}
\end{algorithm}

Computing the subproduct tree takes $O(\Mult(n)\log n)$ operations and
requires an equivalent storage. Having precomputed it, it is immediate
to evaluate a polynomial at the points $x_1,\ldots,x_n$.

\begin{algorithm}
  \caption{Multipoint evaluation}
  \begin{algorithmic}[1]
    \REQUIRE The subproduct tree, $a\in\K[X]/f(X)$.
    \ENSURE $a(x_1),\ldots,a(x_n)$.
    \FOR {$j=1$ \TO $k$}
    \FORALL {$i\in[1,\ldots,2^{j-1}]$}
    \STATE $a_i^{(j)} \la a_i^{(j+1)} \mod f_{i}^{j+1}$;
    \STATE $a_{2i}^{(j)} \la a_i^{(j+1)} \mod f_{2i}^{j+1}$;
    \ENDFOR
    \ENDFOR
    \STATE return $a_1^{(k)},\ldots,a_n^{(k)}$.
  \end{algorithmic}
\end{algorithm}

If $a$ has degree at most $n$, computing the
\index{multipoint~evaluation}multipoint evaluation also takes
$O(\Mult(n)\log n)$ operations using a fast Newton iteration for
Euclidean division.

For the inverse operation, we use the
\index{Lagrange~interpolant}\emph{Lagrange interpolants} 
\begin{equation}
  \label{eq:223}
  s_i = \prod_{j\ne i}\frac{(X-x_j)}{x_i-x_j}
  \text{.}
\end{equation}
They have the property that
\begin{equation}
  \label{eq:224}
  \begin{aligned}
    s_i &\equiv 0 \mod (X-x_j) &\text{if $i\ne j$,}\\
    s_i &\equiv 1 \mod (X-x_i)\text{;}
  \end{aligned}
\end{equation}
so that
\begin{equation}
  \label{eq:225}
  a \equiv \sum_i a(x_i)s_i \mod f
  \text{.}
\end{equation}
The key observation is that
\begin{equation}
  \label{eq:226}
  f' = \sum_{i}\prod_{j\ne i}(X-x_j)
  \text{,}
\end{equation}
hence
\begin{equation}
  \label{eq:227}
  s_i = \frac{\prod_{j\ne i}(X-x_j)}{f'(x_i)}
  \text{.}
\end{equation}

To \index{interpolation}interpolate the polynomial $a$ from the values
$a(x_i)$, we start by computing the values $f'(x_1),
\ldots,\allowbreak f'(x_n)$ by the previous algorithm. Then we
reconstruct $a$ using the subproduct tree.

\begin{algorithm}
  \caption{\label{alg:interp}Interpolation}
  \begin{algorithmic}[1]
    \REQUIRE $a(x_1),\ldots,a(x_n)$, the subproduct tree.
    \ENSURE $a=\sum_i a(x_i)s_i$.
    \STATE compute $f'(x_1),\ldots,f'(x_n)$ using multipoint evaluation;
    \STATE $p_i^{(k)}\la a(x_i)/f'(x_i)$ for $0<i\le2^i$;
    \FOR {$j=k-1$ \TO $0$}
    \FORALL {$i\in[1,\ldots,2^{j}]$}
    \STATE $p_i^{(j)} \la p_i^{(j+1)}t_{2i}^{(j+1)} + p_{2i}^{(j+1)}t_{i}^{(j+1)}$;
    \ENDFOR
    \ENDFOR
    \STATE return $p_0$.
  \end{algorithmic}
\end{algorithm}

Thus, interpolation too can be computed with $O(\Mult(n)\log n)$
operations in $\K$. These algorithms can be generalized to compute the
Chinese remainder isomorphism and its inverse for arbitrary moduli
$f_1,\ldots,f_r\in\K[X]$ such that $\gcd(f_i,f_j)=1$ for $i\ne j$. The
complexity is again $O(\Mult(n)\log n)$, where $n$ is the sum of the
degrees of $f_1,\ldots,f_n$. See~\cite[$\S10$]{vzGG} for details.



\subsection[XGCD, Cauchy interpolation and RFR]{Euclidean algorithm,
  Cauchy interpolation and rational fraction reconstruction}
\label{sec:eucl-algor-rati}
Let $\K$ be a field, given two polynomials $f,g\in\K[X]$ of degrees
$m,n$, the \index{Euclidean~algorithm}Euclidean algorithm permits us to
compute their \index{GCD}GCD using $O(mn)$ operations in $\K$. Let $r$
be the GCD of $f$ and $g$, a \index{Bézout~relation}\emph{Bézout
  relation} is an equation of the form
\begin{equation}
  \label{eq:154}
  fu + gv = r
  \text{,}
\end{equation}
with $u,v\in\K[X]$. If we ask $\deg(ur)<\deg(g)$ and
$\deg(vr)<\deg(a)$, the Bézout relation is unique; computing it is
called the extended GCD problem (\index{XGCD}XGCD) and can be computed
by the \index{extended Euclidean algorithm}\emph{extended Euclidean
  algorithm}.

\begin{algorithm}
  \caption{Extended Euclidean algorithm}
  \begin{algorithmic}[1]
    \REQUIRE $f,g\in\K[X]$.
    \ENSURE $u,v,r\in\K[X]$ such that $fu+gv=r$.
    \STATE let $r_0\la f$, $u_0\la1$, $v_0\la0$;
    \STATE let $r_1\la f$, $u_1\la0$, $v_1\la1$;
    \STATE $i\la1$;
    \WHILE{$r_i\ne0$}
    \STATE compute $r_{i-1} = q_ir_i + r_{i+1}$ by Euclidean division;
    \STATE compute $u_{i+1} \la u_{i-1} - q_iu_i$, $v_{i+1} \la v_{i-1} - q_iv_i$;
    \STATE $i\la i+1$;
    \ENDWHILE
    \STATE return $u_i,v_i,r_i$.
  \end{algorithmic}
\end{algorithm}

One important application of XGCD's is computing modular inverses: let
$f,g\in\K[X]$ with $\deg(g)<\deg(f)$ and $f$ prime to $g$, then $r$ is
a unit in $\K$, and a Bézout relation implies
\begin{equation}
  \label{eq:206}
  g\frac{v}{r} \equiv 1 \mod f
  \text{.}
\end{equation}

More generally, the polynomials computed at each iteration by the
extended Euclidean algorithm satisfy
\begin{equation}
  \label{eq:156}
  fu_i + gv_i =  r_i
  \qquad\text{for any $i$;}
\end{equation}
each of these is also called a Bézout relation. These relations have
two major applications: \emph{Cauchy interpolation} and \emph{rational
  fraction reconstruction}.

Given $n$ pairs $(x,e)\in\K\times\K$ with all $x$ distinct and an
integer $\ell<n$, \index{Cauchy~interpolation}Cauchy interpolation
computes, if it exists, a rational fraction $\frac{r}{v}\in\K(X)$ with
$\deg r<\ell$ and $\deg v \le n-\ell$, such that $\frac{r(x)}{v(x)}=e$
for any $(x,e)$. Let $f=\prod (X-x)$, by interpolation one obtains the
unique polynomial $g\in\K[X]/f$ such that $g(x)=e$ for any
$(x,e)$. Then a Bézout relation for $f$ and $g$ gives
\begin{equation}
  \label{eq:207}
  \frac{r_i}{v_i} \equiv g \mod f
  \qquad\text{for any $i$,}
\end{equation}
thus in particular $\frac{r_i(x)}{v_i(x)}=e$ for any $(x,e)$; this
phase often goes under the name of rational fraction reconstruction
too. It can be proven that a solution to the Cauchy interpolation
problem exists if and only if one of the intermediate results of the
extended Euclidean algorithm is such that $\deg(r_i)<\ell$ and
$\deg(v_i)\le n-\ell$.

\index{rational~fraction~reconstruction}Rational fraction
reconstruction (\index{RFR|see{rational fraction reconstruction}}RFR) is very similar to Cauchy
interpolation, and it can be viewed as a generalization of it using
multiplicities. Let $g\in\K[[X]]$ be a power series, we want to
compute a rational fraction $\frac{r}{v}\in\K(X)$ with $\deg(r)<\ell$
and $\deg(v)\le n-\ell$, such that $\frac{r}{v}=g+O(X^n)$ in
$\K[[x]]$. Such a rational fraction is called a
\index{Padé~approximant}\emph{Padé approximant} of type
$(\ell-1,n-\ell)$ of $g$. Again, it can be shown that a Padé
approximant of type $(\ell-1,n-\ell)$ exists if and only if
\begin{equation}
  \label{eq:210}
  \frac{r_i}{v_i}\equiv g \mod X^{n+m+1}
\end{equation}
is one of the intermediate results computed by the extended Euclidean
algorithm.

The extended Euclidean algorithm is not optimal. We address the reader
to \cite[$\S$11.1]{vzGG} for the description of an algorithm that
takes $f,g\in\K[X]$ of degree at most $n$ and $\ell\le n$, and
computes, using $O(\Mult(n)\allowbreak\log n)$ operations, the rows $u_i,v_i,r_i$
and $u_{i+1},v_{i+1},r_{i+1}$ of the Extended Euclidean algorithm such
that $\deg(r_i)\ge n-\ell$ and $\deg(r_{i+1})<n-\ell$. A consequence
of this algorithm is that both Cauchy interpolation and rational
fraction reconstruction can be computed in $O(\Mult(n)\log n)$
operations.



\subsection{Multivariate polynomials}
\label{sec:mult-polyn}
Computing with multivariate polynomials has many complications,
compared to the univariate case. Part~\ref{part:fast-arithm-using}
will be dedicated to some advanced algorithms for some specific
instances of quotient rings of multivariate polynomials. Here, we just
recall the basic techniques that permit us to reduce the multivariate
to the univariate case.

\pdfmcthree{Cited Kaltofen.}  Multiplication of polynomials in
$R[X,Y]$ can be reduced to univariate multiplication by
\index{Kronecker~substitution}\emph{Kronecker
  substitution}~\cite{moenck76,kaltofen87,vzGG,vzgathen+shoup92,harvey09}. If
$f,g\in R[X,Y]$ have degree at most $m$ in $X$ and at most $n$ in $Y$,
the product $fg$ can be computed as
\begin{equation}
  \label{eq:222}
  fg \equiv f(X,X^{2m-1})\cdot g(X,X^{2m-1}) \mod Y-X^{2m-1}
  \text{.}
\end{equation}
Observing that the reduction modulo $Y-X^{2m-1}$ comes for free, the
cost of the previous computation is $O(\Mult(mn))$.

% Overfull in a5
Kronecker substitution also allows to do multiplication in
$\bigl(\K[X]/f(X)\bigr)[Y]$ by multiplying in $\K[X,Y]$ first and then
reducing modulo $f$. The $O(\Mult(mn))$ operations in $\K$ ($m$ being
the degree of $f$ and $n$ being a bound on the degree in $Y$). The
same idea can be applied to multiply elements of $\K[X,Y]/I$, where $I$
is a \emph{triangular ideal}
\begin{equation}
  \label{eq:228}
  I = \left\langle f(X), g(X,Y)\right\rangle
  \qquad\text{with $g$ monic in $Y$,}
\end{equation}
using $O(\Mult(mn))$ operations, where $m=\deg f$ and $n=\deg_Y g$;
the algorithm and the complexity analysis can be found
in~\cite[Proposition~4]{pascal+schost06}.

All the algorithms presented previously that use multiplication as a
black box, can be generalized to the bivariate case using Kronecker
substitution. Thus, for example, Euclidean division in
$\left(\K[X]/f(x)\right)[Y]$ can be done in $O(\Mult(mn))$ operations,
inversion in $\K[X,Y]/I$ in $O(\Mult(mn)\log mn)$ operations, etc.

\pdfmcone{Cowardly removed false statement about known
  algorithms for multivariate multiplication.}  Unfortunately,
Kronecker substitution does not scale well with the number of
variables. For an analysis of the problem and alternatives,
see~\cite{schost05,li+moreno+schost07}. In general, no quasi-linear
time algorithm is known to multiply elements of a finite dimensional
algebra $\K[X_1,\ldots,X_n]/I$, even in the case $I$ is triangular.


\subsection{Transposed algorithms}
\label{sec:transp-algor}
Finally, we shall recall some known results about
\index{transposed~algorithm}\emph{transposed algorithms}. The theory
of transposition will be studied in detail in
Part~\ref{part:transp-princ}; for the moment we shall just recall that
an algorithmic principle, known as the
\index{transposition~principle}\emph{transposition
  principle}~\cite{shoup94,shoup95,shoup99,Ka2K,hanrot+quercia+zimmermann,bostan+lecerf+schost:tellegen},
states that
\begin{quote}
  Let $\pspace$ be an arbitrary set. To any $R$-algebraic algorithm
  $A$ computing a family of linear functions $(f_p:M\ra
  N)_{p\in\pspace}$ corresponds an $R$-algebraic algorithm $\dual{A}$
  computing the \emph{dual family}
  $(\dual{f}_p:\dual{N}\ra\dual{M})_{p\in\pspace}$. The algebraic time
  and space complexities of $\dual{A}$ are bounded by the time
  complexity of $A$.
\end{quote}

In Part~\ref{part:transp-princ} we shall see that this principle has
an algorithmic content and that the \emph{dual} algorithm can be
inferred automatically. For the moment, we are just interested in its
existential aspect. We consider two problems for which the transpose
problem has been studied.

\paragraph{Transposed multiplication}
\label{sec:transp-mult}
The first one is \index{transposed~multiplication}polynomial
multiplication. Let $a\in R[X]$, the map $M_a:b\mapsto ab$ is a linear
map. As usual, we identify the dual module of $R[X]$ to $R[[1/X]]$ via
the bilinear form
\begin{equation}
  \label{eq:276}
  \braket{\alpha}{b} = [\alpha b]_0
  \text{,}
\end{equation}
where $\alpha\in R[[1/X]]$, $b\in R[X]$ and
$[\beta]_i$ is the coefficient of
$X^i$ in $\beta$.

Then, the dual map to $M_a$ is given by
\begin{equation}
  \label{eq:229}
  \braketop{\alpha}{M_a}{b} = \braket{\alpha}{ab} = [\alpha ab]_0 =
  \braket{a\cdot\alpha}{b} = \braket{\dual{M_a}(\alpha)}{b}
  \text{,}
\end{equation}
where the product $a\cdot\alpha$ is defined as the usual product,
with coefficients of $X^i$ discarded if $i>0$.

In particular, let $a$ have degree $m$. If we restrict $M_a$ to
$R[X]_n$ (the polynomials of degree at most $n$), the image of $M_a$
is in $R[X]_{m+n}$. Then, we identify $\dual{(R[X]_{m+n})}$ to
$R[1/X]_{m+n}$ and $\dual{(R[X]_n)}$ to $R[1/X]_n$. 

Hence, the map 
\begin{equation}
  \label{eq:231}
  \begin{aligned}
    \dual{M_a}:R[1/X]_{m+n}&\ra R[1/X]_n\text{,}\\
    \alpha&\mapsto a\cdot\alpha  
  \end{aligned}
\end{equation}
is defined by truncating the power series at $1/X^n$:
\begin{equation}
  \label{eq:230}
  a\cdot\alpha = \sum_{i=0}^ma_iX^i \cdot \sum_{j=0}^{m+n}\frac{\alpha_j}{X^j} =
  \sum_{k=0}^{n}\sum_{i-j=k}\frac{a_i\alpha_j}{X^k}
  \text{.}
\end{equation}
Observe that the coefficients of $a\cdot\alpha$ are the same as the
coefficients of $a(\alpha X^{m+n})$ between $X^{m}$ and $X^{m+n}$,
thus any algorithm for polynomial multiplication can be used to
compute $a\cdot\alpha$ in $\Mult(2m+n)$ operations. This is the reason
why \emph{transposed polynomial multiplication} is also called
\index{middle~product}\emph{middle product} in the
literature~\cite{bostan+lecerf+schost:tellegen,hanrot+quercia+zimmermann}.
The generalization to the multivariate case is straightforward.

Observe, however, that this algorithm has nothing to do with the
transposition principle: it is just a property of
multiplication. Applying the transposition principle, one obtains a
tighter bound of $\Mult(\max(m,n))$. The univariate case is treated
in~\cite{hanrot+quercia+zimmermann,bostan+lecerf+schost:tellegen}, it
is applied to speed up some Newton iterations on power series; the
bivariate case appears in the proof
of~\cite[Corollary~2]{pascal+schost06}.

\paragraph{Transposed Euclidean division}
\label{sec:transp-eucl-divis}
{\ifafourps\postdisplaypenalty=100\fi
We now study the dual of Euclidean division with remainder; it is
actually only the remainder that we are interested in. Let $f$ be a
monic polynomial in $R[X]$, the map 
\begin{equation}
  \label{eq:238}
  \begin{aligned}
    \bmod_f:R[X]&\ra R[X]_n\text{,}\\
    a&\mapsto a\bmod f
  \end{aligned}
\end{equation}
is a linear map.}

Suppose $f$ has degree $n+1$, then the dual map
\begin{equation}
  \label{eq:233}
  \dual{\bmod_f}:R[1/X]_n\ra R[[1/X]]  
\end{equation}
is such that, for any $\alpha\in\R[X]_n$ and any $i\ge0$,
\begin{equation}
  \label{eq:234}
  \braket{\beta}{X^if} \eqdef \braket{\dual{\bmod_f}(\alpha)}{X^if} = 
  \braket{\alpha}{X^if\bmod f} = 0
  \text{,}
\end{equation}
where we have set $\beta\eqdef\dual{\bmod_f}(\alpha)$.

Using what we saw previously on transposed multiplication,
\begin{equation}
  \label{eq:235}
  \braket{X^i\cdot\beta}{f} = \braket{\beta}{X^if} = 0
  \quad\text{for any $i\ge0$.}
\end{equation}
Equivalently, the coefficients of $\beta$ satisfy a linear recurrence
with minimal polynomial $f$. If
\begin{equation}
  \label{eq:236}
  \alpha = \sum_{i=0}^{n}\frac{\alpha_i}{X^i}
  \text{,}\qquad
  \beta = \sum_{i\ge0}\frac{\beta_i}{X^i}
\end{equation}
the initial conditions for $\beta$ are
\begin{equation}
  \label{eq:237}
  \beta_i = \braket{\beta}{X^i} = \braket{\alpha}{X^i\bmod f} = \braket{\alpha}{X^i} = \alpha_i
\end{equation}
for any $i\le n$.

\pdfmcone{Cited Shoup 99.}  Thus, the dual of modular reduction
consists in extending a linear recurring sequence of order $n$ from
its first $n$ elements. Any algorithm for such task (for
example,~\cite[$\S$3]{shoup99}) can be used to compute
$\dual{\bmod_f}$, however one can also directly obtain one such
algorithm by applying the transposition principle to any algorithm for
Euclidean division. In any case,
\index{transposed~modular~reduction}\emph{transposed modular
  reduction} to the order $N$ can be computed using $O(\Mult(N))$
operations\footnote{One can do better when $N$ is much larger than
  $n$, but we shall not need such an improvement in this document.}.

Finally, the composition $\bmod_f\circ M_a$ gives an algorithm for
multiplication in the ring $R[X]/f(X)$. Hence, the dual to this is
simply $\dual{M_a}\circ\dual{\bmod_f}$. This is called
\index{transposed~modular~multiplication}\emph{transposed modular
  multiplication} and we shall use this repeatedly in the next
chapters. The generalization of this to the case of $\K[X,Y]/I$, where
$I$ is a triangular ideal, is straightforward but technical; we
address to~\cite[Corollary~2]{pascal+schost06} for the details.



%%% Local Variables: 
%%% mode:flyspell
%%% ispell-local-dictionary:"american"
%%% mode: TeX-PDF
%%% mode: reftex
%%% TeX-master: "../these"
%%% End: 
