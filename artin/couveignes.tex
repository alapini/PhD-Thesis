\section{Arbitrary towers}
\label{sec:couveignes-algorithm}

Finally, we bring our previous algorithms to an arbitrary tower, using
Couveignes' isomorphism algorithm~\cite{couveignes00}. As in the
previous section, we adapt this algorithm to our context, by adding
suitable push-down and lift-up operations.

Let $Q_0$ be irreducible of degree $d$ in $\F_p[X_0]$, such that
$\Tr_{\U_0/\F_p}(x_0)\ne0$, with as before
$\U_0=\F_p[X_0]/Q_0$. We let $(G_i)_{0 \le i < k}$ and
$(\U_0,\ldots,\U_k)$ be as in Section~\ref{sec:fast-tower}.

We also consider another sequence $(G'_i)_{0 \le i < k}$, that defines
another tower $(\U'_0,\ldots,\U'_k)$.  Since $(\U'_0,\ldots,\U'_k)$ is
not necessarily primitive, we fall back to the multivariate basis of
Subsection~\ref{ssec:rep}: we write elements of $\U'_i$ on the basis
  \begin{equation}
  \label{eq:58}
  \basis{B'}_i=
  \{{x'_0}^{e_0} \cdots {x'_i}^{e_i} \;|\; 0 \le e_0 < d,\; 0\le e_j < p 
  \text{ for $j>0$}\}
  \text{,}
\end{equation}
where $x_0=x'_0$.
%% We assume that all
%% polynomials $G'_i$ satisfy $\deg(G'_i,X_0)< d$ and $\deg(G'_i,X_j)< p$
%% for $j \le i$, so that writing $\gamma'_i$ on the basis $\bB'_i$
%% requires no operation.

To compute in $\U'_i$, we will use an isomorphism $\U'_i \ra \U_i$.
Such an isomorphism is determined by the images
$\lst{s}_i=(s_0,\dots,s_i)$ of $(x'_0,\dots,x'_i)$, with $s_i \wrt
\U_i$ (we always take $s_0=x_0$). This isomorphism, denoted by
$\sigma_{\lst{s}_i}$, takes as input $v$ written on the basis
$\basis{B}'_i$ and outputs $\sigma_{\lst{s}_i}(v)\wrt \U_i$.

To analyze costs, we use the functions $\Lift$ and $\Ptr$ introduced
in the previous sections. We also let $2 \le \omega \le 3$ be a
feasible exponent for linear algebra over $\F_p$ (see
Section~\ref{sec:todo}).
\begin{theorem}\label{theo:main}
  Given $Q_0$ and $(G'_i)_{0 \le i < k}$, one can find
  $\lst{s}_k=(s_0,\dots,s_k)$ in time $O(d^\omega k + \Ptr(k) +
  \Mult(p^{k+1} d) \log(p))$. Once they are known, one can apply
  $\sigma_{\lst{s}_k}$ and $\sigma_{\lst{s}_k}^{-1}$ in time $O(k\,
  \Lift(k))$.
\end{theorem}
Thus, we can compute products, inverses, etc, in $\U'_k$ for
the cost of the corresponding operation in $\U_k$, plus $O(k\,
\Lift(k))$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Solving Artin-Schreier equations} 

As a preliminary, given $\alpha\wrt \U_i$, we discuss how to
solve the Artin-Schreier equation $X^p-X=\alpha$ in $\U_i$. We assume
that $\Tr_{\U_i/\F_p}(\alpha)=0$, so this equation has solutions in
$\U_i$.

Because $X^p-X$ is $\F_p$-linear, the equation can be directly solved
by linear algebra, but this is too costly. In~\cite{couveignes00},
Couveignes gives a solution adapted to our setting, that reduces the
problem to solving Artin-Schreier equations in $\U_0$. Given a solution
$\delta\in\U_i$ of the equation $X^p - X = \alpha$, he observes that 
any solution $\mu$ of
\begin{equation}
  \label{eq:approximateAS}
  X^{p^{p^{i-1}d}} - X = \eta, \quad\text{with}\quad \eta=\PTr_{p^{i-1}d}(\alpha).
\end{equation}
is of the form $\mu=\delta - \Delta$ with $\Delta\in\U_{i-1}$, hence
$\Delta$ is a root of
\begin{equation}
  \label{eq:approximant}
  X^p-X-\alpha+\mu^p-\mu.
\end{equation}
This equation has solutions in $\U_{i-1}$ by hypothesis and hence it
can be solved recursively. First, however, we tackle the problem of
finding a solution of~\eqref{eq:approximateAS}.

For this purpose, observe that the left hand side
of~\eqref{eq:approximateAS} is $\U_{i-1}$-linear and its matrix on the
basis $(1,\ldots,x_i^{p-1})$ is
\begin{equation*}
  \label{eq:approximate-matrix}
  \begin{bmatrix}
    0 & \binom{1}{0}\beta_{i-1,p^{i-1}d} & \hdots & \binom{p-1}{0}\beta_{i-1,p^{i-1}d}^{p-1} \\
      & \ddots          &        & \vdots               \\
      &                 & 0      &\binom{p-1}{p-2}\beta_{i-1,p^{i-1}d} \\
      &                 &        & 0
  \end{bmatrix}
\end{equation*}
Then, algorithm \alg{ApproximateAS} finds the required solution.


%% We use an algorithm called \alg{Summation}; given $E$ in
%% $\U_{i-1}[X]$ of degree less than $p-1$ and $\beta$ in $\F_p$, it
%% returns the unique $D$ in $\U_{i-1}[X]$ of degree less than $p$ such
%% that $D(X+\beta)-D(X)=E(X)$ and $D(0)=0.$ Rescaling by $\beta$, we
%% can assume that $\beta=1$. Then, the algorithm converts to the falling
%% factorial basis, sums and converts back~\cite[Ch.~21]{vzGG}, using
%% $O(p^2)$ operations $(+,\times)$ in $\U_{i-1}$ (better solutions are
%% known, but do not improve the overall runtime in Theorem~\ref{th:approximateAS}).


\begin{algorithm}
  \caption{ApproximateAS} 
  \begin{algorithmic}[1]
    \REQUIRE $\eta\wrt\U_i$ such that~\eqref{eq:approximateAS} has a solution.
    \ENSURE $\mu\wrt\U_i$ solution of~\eqref{eq:approximateAS}.
    \STATE let $\eta_0 + \eta_1 x_i + \dots + \eta_{p-2} x_i^{p-2}=\text{\alg{Push-down}}(\eta)$
    \FORALL {\label{alg:AAS:loop} $j\in[p-1,\ldots,1]$}
    \STATE let $\mu_j =
   \frac{1}{jT}\left(\eta_{j-1} -
  \sum_{h=j+1}^{p-1}\binom{h}{j-1}\beta_{i-1,p^{i-1}d}^{h-j+1}\mu_h\right)$
\ENDFOR
\STATE return $\text{\alg{Lift-up}}(\mu_1 x_i + \ldots + \mu_{p-1} x_i^{p-1})$
\end{algorithmic}
\end{algorithm}

%% \begin{itemize}
%%   {ApproximateAS} 
%%   {$\eta\wrt\U_i$ such that~\eqref{eq:approximateAS} has a solution.}
%%   {$\mu\wrt\U_i$ solution of~\eqref{eq:approximateAS}.}
%% \STATE let $\eta_0 + \eta_1 x_i + \dots + \eta_{p-2} x_i^{p-2}=\text{\alg{Push-down}}(\eta)$
%% \STATE \label{alg:AAS:loop}for $j\in[p-1,\ldots,1]$,\\ let $\mu_j =
%%    \frac{1}{jT}\left(\eta_{j-1} -
%%   \sum_{h=j+1}^{p-1}\binom{h}{j-1}\beta_{i-1,p^{i-1}d}^{h-j+1}\mu_h\right)$
%% \STATE return $\text{\alg{Lift-up}}(\mu_1 x_i + \ldots + \mu_{p-1} x_i^{p-1})$
%% \end{itemize}

\begin{theorem}
  \label{th:approximateAS}
  Algorithm \alg{ApproximateAS} is correct and takes time $O(\Lift(i))$.
\end{theorem}

\begin{proof}
  Correctness is clear by Gaussian elimination.  For the cost
  analysis, remark that $\beta_{i-1,p^{i-1}d}$ has already been
  precomputed to permit iterated Frobenius and pseudotrace
  computations. Step~\ref{alg:AAS:loop} takes $O(p^2)$ additions and
  scalar operations in $\U_{i-1}$; the overall cost is dominated by
  that of the push-down and lift-up by assumptions on $\Lift$.
\end{proof}

%% \begin{proof} Write $\mu=\Sigma_{j=0}^{p-1}\mu_j x_i^j$ and
%% $\eta=\Sigma_{j=0}^{p-1}\eta_j x_i^j$, with $\mu_j$ and $\eta_j \wrt
%% \U_{i-1}$. Since all $\mu_j$ are invariant by the $p^{i-1}d$-power of
%% the Frobenius, we get from Equation~\eqref{eq:frobeniussum}
%% \begin{equation*}
%%     \begin{array}{c}
%% {\mu}^{p^{p^{i-1}d}}=\sum_{j=0}^{p-1} \mu_j x_i^{jp^{p^{i-1}d}} = \sum_{j=0}^{p-1}  \mu_j(x_i +\beta_{i-1,p^{i-1}d}))^j,
%%     \end{array}
%% \end{equation*}
%% where $\beta_{i-1,p^{i-1}d}$ is the trace of $\gamma_{i-1}$ over $\F_p$.
%% Thus, $\mu$ is a solution of~\eqref{eq:approximateAS} if and only if
%% the polynomials $D=\sum_{j=0}^{p-1} \mu_j X^j$ and $E=\sum_{j=0}^{p-1}
%% \eta_j X^j$ satisfy $D(X+\beta_{i-1,p^{i-1}d}) - D(X) = E(X).$ Remark that
%% $\eta_{p-1}$ must be $0$ and that $\mu_0$ plays no role in the
%% solution, so we take $\mu_0=0$. For the cost analysis, remark that
%% $\beta_{i-1,p^{i-1}d}$ has already been precomputed to permit iterated
%% Frobenius and pseudotrace computations (see Section
%% \ref{sec:pseudotrace-frobenius}). Then, the overall cost is dominated
%% by that of the push-down and lift-up by assumptions on $\Lift$. \end{proof}


Writing the recursive algorithm is now straightforward. To solve
Artin-Schreier equations in $\U_0$, we use a naive algorithm based on
linear algebra, written $\alg{NaiveSolve}$.

\begin{algorithm}
  \caption{Artin-Schreier}
  \begin{algorithmic}[1]
    \REQUIRE $\alpha,i$ such that $\alpha\wrt\U_i$ and $\Tr_{\U_i/\F_p}(\alpha)=0$.
    \ENSURE $\delta\wrt\U_i$ such that $\delta^p-\delta=\alpha$.
    \STATE \label{alg:cou:base}if $i=0$, return $\alg{NaiveSolve}(X^p-X-\alpha)$
    \STATE \label{alg:cou:pseudo} let $\eta = \alg{Pseudotrace}(\alpha, i,i-1)$
    \STATE \label{alg:cou:push-beta} let $\mu= \alg{ApproximateAS}(\eta)$
    \STATE \label{alg:cou:push-alpha} let $\alpha_0=\text{\alg{Push-down}}(\alpha-\mu^p+\mu)$
    \STATE \label{alg:cou:rec} let $\Delta =\text{\alg{Artin-Schreier}}(\alpha_0,i-1)$
    \STATE \label{alg:cou:lift} return $\mu+\text{\alg{Lift-up}}(\Delta)$
  \end{algorithmic}
\end{algorithm}

\begin{theorem}\label{theo:AS}
  Algorithm \alg{Artin-Schreier} is correct and takes time $O(d^\omega
  + \Ptr(i))$.
\end{theorem}
\begin{proof} 
  Correctness follows from the previous discussion.  For the
  complexity, note ${\sf AS}(i)$ the cost for $\alpha\wrt\U_i$. The
  cost ${\sf AS}(0)$ of the naive algorithm is $O(\Mult(d)\log(p) +
  d^\omega)$, where the first term is the cost of computing $x_0^p$
  and the second one the cost of linear algebra.

  When $i\ge1$, step \ref{alg:cou:pseudo} has cost $\Ptr(i)$, steps
  \ref{alg:cou:push-beta}, \ref{alg:cou:push-alpha} and
  \ref{alg:cou:lift} all contribute $O(\Lift(i))$ and step
  \ref{alg:cou:rec} contributes ${\sf AS}(i-1)$. The most important
  contribution is at step \ref{alg:cou:pseudo}, hence ${\sf AS}(i) =
  {\sf AS}(i-1) + O(\Ptr(i))$. The assumptions on $\Lift$ imply that
  the sum $\Ptr(1) + \cdots + \Ptr(i)$ is $O(\Ptr(i))$.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Applying the isomorphism}

\subsubsection{Duality}
\label{ssec:duality}

Since this is the case we will need later on, we give
details for the case where $Q$ is Artin-Schreier (so $n=p$): then,
$Q'=-1$, so no work is needed to invert it modulo $Q$.

In the following algorithm, we suppose that $\U'$ is presented as
$\U'=\U[X]/P$, where $P$ is Artin-Schreier. We let $x$ be the residue
class of $X$ in $\U'$.
\begin{algorithm}
  \caption{FindParameterization}
  \begin{algorithmic}[1]
    \REQUIRE $w \in \U'$ written as $w_0 + \cdots + w_{p-1} x^{p-1}$,  
    $b \in \U'$ written as $b_0 + \cdots + b_{p-1} x^{p-1}$
    \ENSURE A polynomial $A$ of degree less than $p$ such that $w=A(b)$
    \STATE\label{alg:para:trmul} let $\ell = w \cdot\Tr_{\U'/\U}$
    \STATE\label{alg:para:trmodcomp} let $M= \sum_{j < p}\ell(b^j)X^j$
    \STATE\label{alg:para:multrunc} let $N = M \rev_{p}(Q) {\sf ~mod~} X^{p}$
    \STATE\label{alg:para:mulmod} return $-\rev_{p-1}(N)$
  \end{algorithmic}
\end{algorithm}

\begin{proposition}
  \label{th:findparameterization}
  If $Q$ is Artin-Schreier, the cost of $\alg{FindParameterization}$ is
  $O(p^2)$ operations $(+,\times)$ in $\U$.
\end{proposition}
\begin{proof} By~\ref{eq:pd}, the representation of $\Tr_{\U'/\U}$ in
$\U'^\ast$ is simply $(0,\ldots,0,-1)$. Then by the discussion above,
if $T$ is the cost of multiplying two elements of $\U'$ in the basis
$(1,\ldots,x^{p-1})$, step~\ref{alg:para:trmul} costs $T + O(p)$; this
stays in $O(p^2)$ by taking a naive
multiplication. Step~\ref{alg:para:trmodcomp} fits into the same
bound, by the proof of~\cite[Th.~4]{Sho94}. Taking the $\rev$'s in
steps~\ref{alg:para:multrunc} and~\ref{alg:para:mulmod} is just
reading the polynomials from right to left, thus this costs no
arithmetic operation. Finally, step~\ref{alg:para:multrunc} features a
polynomial multiplication truncated to the order $p$, this costs
$O(p^2)$ operations by a naive algorithm.\end{proof}

Note that this cost can be improved with respect to $p$, by using fast
modular composition as in~\cite{Sho94}; we do not give details, as this
would not improve the overall complexity of the algorithms of the next
sections.

%%%%%%%%%%%%%%%%%%%55

We get back to the isomorphism question. We assume that
$\lst{s}_i=(s_0,\dots,s_i)$ is known and we give the cost of applying
$\sigma_{\lst{s}_i}$ and its inverse.  We first discuss the forward
direction.

As input, $v \in \U'_i$ is written on the multivariate basis $\basis{B}'_i$
of $\U'_i$; the output is $t=\sigma_{\lst{s}_i}(v) \wrt \U_i$. As before,
the algorithm is recursive: we write $v=\Sigma_{j <p}
v_j(x'_0,\dots,x'_{i-1}) {x'_i}^j$, whence
\begin{equation}
  \sigma_{\lst{s}_i}(v)\ =\ \sum_{j
    <p} \sigma_{\lst{s}_i}(v_j) s_i^j\ =\ \sum_{j
    <p} \sigma_{\lst{s}_{i-1}}(v_j) s_i^j
  \text{;}
\end{equation}
the sum is computed by Horner's scheme.  To speed-up the computation,
it is better to perform the latter step in a bivariate basis, that is,
through a push-down and a lift-up.

Given $t \wrt \U_i$, to compute $v=\sigma_{\lst{s}_i}^{-1}(t)$, we run
the previous algorithm backward. We first push-down $t$, obtaining
$t=t_0 + \cdots + t_{p-1}x_i^{p-1}$, with all $t_j \wrt
\U_{i-1}$. Next, we rewrite this as $t=t'_0+\cdots +
t'_{p-1}s_i^{p-1}$, with all $t'_j \wrt \U_{i-1}$, and it suffices to
apply $\sigma_{\lst{s}_i}^{-1}$ (or equivalently
$\sigma_{\lst{s}_{i-1}}^{-1}$) to all $t'_i$. The non-trivial part is
the computation of the $t'_j$: this is done by applying the algorithm
\alg{FindParameterization} above, in the extension $\U_i=
\U_{i-1}[X_i]/P_i$.


\begin{algorithm}
  \caption{ApplyIsomorphism} 
  \begin{algorithmic}[1]
    \REQUIRE $v,i$ with $v\in \U'_i$ written on the basis $\basis{B}'_i$.
    \ENSURE $\sigma_{\lst{s}_i}(v) \wrt \U_i$.
    \STATE if $i=0$ then return $v$
    \STATE write $v=\Sigma_{j <p} v_j(x'_0,\dots,x'_{i-1}) {x'_i}^j$
    \STATE let $s_{i,0}+\cdots+s_{i,p-1}x_i^{p-1}=\text{\alg{Push-down}}(s_i)$
    \STATE for $j \in [0,\dots,p-1]$ let $t_j=\alg{ApplyIsomorphism}(v_j,i-1)$
    \STATE let $t=0$
    \STATE  for $j \in [p-1,\dots,0]$ let $t=(s_{i,0}+\cdots+s_{i,p-1}x_i^{p-1})t+t_j$
    \STATE return $\text{\alg{Lift-up}}(t)$
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}
  \caption{ApplyInverse} 
  \begin{algorithmic}[1]
    \REQUIRE $t,i$ with $t \wrt \U_i$.
    \ENSURE $\sigma_{\lst{s}_i}^{-1}(t)\in \U'_i$ written on the basis $\basis{B}'_i$.
    \STATE if $i=0$ then return $t$
    \STATE let $t_0 + \cdots + t_{p-1}x_i^{p-1} = \text{\alg{Push-down}}(t)$
    \STATE let $s_{i,0}+\cdots+s_{i,p-1}x_i^{p-1}=\text{\alg{Push-down}}(s_i)$
    \STATE let $t'_0 + \cdots + t'_{p-1}X^{p-1} = \alg{FindParameterization}
  (t_0 + \cdots + t_{p-1}x_i^{p-1}, s_{i,0}+\cdots+s_{i,p-1}x_i^{p-1})$
  \STATE return $\Sigma_{j < p} \alg{ApplyInverse}(t'_j, i-1) {x'_i}^j$
\end{algorithmic}
\end{algorithm}


\begin{proposition}\label{Prop:apply}
  Algorithms \alg{ApplyIsomorphism} and \alg{ApplyInverse} are correct
  and both take time $O(i\Lift(i))$.
\end{proposition}
\begin{proof}
  In both cases, correctness is clear, since the algorithms translate
  the former discussion. As to complexity, in both cases, we do $p$
  recursive calls, $O(1)$ push-downs and lift-ups, and a few extra
  operations: for \alg{ApplyIsomorphism}, these are $p$
  multiplications / additions in the bivariate basis $\basis{D}_i$ of
  Section~\ref{sec:level-embedding}; for \alg{ApplyInverse}, this is
  calling the algorithm \alg{FindParameterization} above.  The costs
  are $O(p\Mult(p^id))$ and $O(p^2\Mult(p^{i-1}d))$, which are in
  $O(\Lift(i))$ by assumption on $\Lift$. We conclude as in
  Theorem~\ref{th:b-ifrob}.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Proof of Theorem~\ref{theo:main}}

Finally, assuming that only $(s_0,\dots,s_{i-1})$ are known, we
describe how to determine $s_i$. Several choices are possible: the
only constraint is that $s_i$ should be a root of
$X_i^p-X_i-\sigma_{\lst{s}_i}(\gamma'_{i-1})=X_i^p-X_i-\sigma_{\lst{s}_{i-1}}(\gamma'_{i-1})$
in $\U_i$.

Using Proposition~\ref{Prop:apply}, we can compute
$\alpha=\sigma_{\lst{s}_{i-1}}(\gamma'_{i-1}) \wrt\U_{i-1}$ in time
$O((i-1)\Lift(i-1)) \subset O(i\Lift(i))$.  Applying a lift-up to $\alpha$,
we are then in the conditions of Theorem~\ref{theo:AS}, so we can find
$s_i$ for an extra $O(d^\omega + \Ptr(i))$ operations.

We can then summarize the cost of all precomputations: to the cost of
determining $\lst{s}_i$, we add the costs related to the tower
$(\U_0,\dots,\U_i)$, given in
Sections~\ref{sec:fast-tower},~\ref{sec:level-embedding}
and~\ref{sec:pseudotrace-frobenius}. After a few simplifications, we
obtain the upper bound $O( d^\omega + \Ptr(i) + \Mult(p^{i+1} d)
\log(p)).$ Summing over $i$ gives the first claim of the theorem. The
second is a restatement of Proposition~\ref{Prop:apply}.

%% ----AS-----   -----------------build------------------  ------level--------   -PT-
%% d^3 + PT(i) + M(p^i+2 d) log(p) + p^i+2 d log_p(p^i d) + M(p^id) \log(p^id) + PT(i)

% Local Variables:
% mode:flyspell
% ispell-local-dictionary:"american"
% mode: TeX-PDF
% mode: reftex
% TeX-master: "../these"
% End:
%
% LocalWords:  Schreier Artin pseudotrace frobenius bivariate memoization
% LocalWords:  precomputed precomputation precompute precomputations Couveignes
