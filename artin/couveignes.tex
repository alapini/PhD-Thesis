%% these.tex
%% Copyright 2010 Luca De Feo
%% All rights reserved


\section{Arbitrary towers}
\label{sec:couveignes-algorithm}

Finally, we bring our previous algorithms to an arbitrary tower, using
Couveignes' isomorphism algorithm~\cite{couveignes00}. As in the
previous section, we adapt this algorithm to our context, by adding
suitable push-down and lift-up operations.

Let $Q_0$ be irreducible of degree $d$ in $\F_p[X_0]$, such that
$\Tr_{\U_0/\F_p}(x_0)\ne0$, with as before
$\U_0=\F_p[X_0]/Q_0$. We let $(G_i)_{0 \le i < k}$ and
$(\U_0,\ldots,\U_k)$ be as in Section~\ref{sec:fast-tower}.

We also consider another sequence $(G'_i)_{0 \le i < k}$, that defines
another tower $(\U'_0,\ldots,\U'_k)$.  Since $(\U'_0,\ldots,\U'_k)$ is
not necessarily primitive, we fall back to the multivariate basis of
Subsection~\ref{ssec:rep}: we write elements of $\U'_i$ on the basis
  \begin{equation}
  \basis{B'}_i=
  \{{x'_0}^{e_0} \cdots {x'_i}^{e_i} \;|\; 0 \le e_0 < d,\; 0\le e_j < p 
  \text{ for $j>0$}\}
  \text{,}
\end{equation}
where $x_0=x'_0$.

To compute in $\U'_i$, we will use an isomorphism $\U'_i \ra \U_i$.
Such an isomorphism is determined by the images
$\lst{s}_i=(s_0,\dots,s_i)$ of $(x'_0,\dots,x'_i)$, with $s_i \wrt
\U_i$ (we always take $s_0=x_0$). This isomorphism, denoted by
$\sigma_{\lst{s}_i}$, takes as input $v$ written on the basis
$\basis{B}'_i$ and outputs $\sigma_{\lst{s}_i}(v)\wrt \U_i$.

To analyze costs, we use the functions $\Lift$ and $\Ptr$ introduced
in the previous sections. We also let $2 \le \omega \le 3$ be a
feasible exponent for linear algebra over $\F_p$ (see
Section~\ref{sec:asympt-compl}).
\begin{theorem}\label{theo:main}
  Given $Q_0$ and $(G'_i)_{0 \le i < k}$, one can find
  $\lst{s}_k=(s_0,\dots,s_k)$ in time 
  \begin{equation}
    \label{eq:105}
    O(d^\omega k + \Ptr(k) +
    \Mult(p^{k+1} d) \log(p))
    \text{.}
  \end{equation}
  Once they are known, one can apply
  $\sigma_{\lst{s}_k}$ and $\sigma_{\lst{s}_k}^{-1}$ in time $O(k\,
  \Lift(k))$.
\end{theorem}
Thus, we can compute products, inverses, etc, in $\U'_k$ for
the cost of the corresponding operation in $\U_k$, plus $O(k\,
\Lift(k))$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Solving Artin-Schreier equations} 

As a preliminary, given $\alpha\wrt \U_i$, we discuss how to
solve the Artin-Schreier equation $X^p-X=\alpha$ in $\U_i$. We assume
that $\Tr_{\U_i/\F_p}(\alpha)=0$, so this equation has solutions in
$\U_i$.

Because $X^p-X$ is $\F_p$-linear, the equation can be directly solved
by linear algebra, but this is too costly. In~\cite{couveignes00},
Couveignes gives a solution adapted to our setting, that reduces the
problem to solving Artin-Schreier equations in $\U_0$. He observed
what follows.

\begin{proposition}
  Let $\delta\in\U_i$ be a solution of the equation 
  \begin{equation}
    \label{eq:106}
    X^p - X = \alpha
    \text{.}    
  \end{equation}
  Any solution $\mu$ of
  \begin{equation}
    \label{eq:approximateAS}
    X^{p^{p^{i-1}d}} - X = \eta, \quad\text{with}\quad \eta=\PTr_{p^{i-1}d}(\alpha).
  \end{equation}
  is of the form $\mu=\delta - \Delta$ with $\Delta\in\U_{i-1}$.
\end{proposition}
\begin{proof}
  Let $\mu$ be a solution of Eq.~\eqref{eq:approximateAS}, and let
  $\Delta=\delta-\mu$, then
  \begin{multline}
    \label{eq:108}
    \Delta^{p^{p^{i-1}d}} - \Delta = \delta^{p^{p^{i-1}d}} - \delta - \eta =
    \left(\sum_{\ell=0}^{p^{i-1}d-1}(\delta^{p}-\delta)^{p^\ell}\right) - \eta =\\
    \PTr_{p^{i-1}d}(\delta^p-\delta) - \eta = 0
    \text{,}
  \end{multline}
  where the last equality comes from the definition of the
  pseudotrace. This implies $\Delta\in\U_{i-1}$.
\end{proof}

By its definition, $\Delta=\delta-\mu$ is a root of
\begin{equation}
  \label{eq:approximant}
  X^p-X-\alpha+\mu^p-\mu.
\end{equation}
This equation has solutions in $\U_{i-1}$ by the previous proposition,
hence it can be solved recursively. In a certain sense, $\mu$ is a
good approximation to $\delta$, and their difference $\Delta$ can be
computed as the solution of a new, simpler, Artin-Schreier equation.

First we tackle the problem of finding a solution
of~\eqref{eq:approximateAS}.  For this purpose, observe that the left
hand side of~\eqref{eq:approximateAS} is $\U_{i-1}$-linear and its
matrix on the basis $(1,\ldots,x_i^{p-1})$ is
\begin{equation}
  \pdfmctwo{More entries in the matrix.}
  \label{eq:approximate-matrix}
  \begin{bmatrix}
    0 & \binom{1}{0}\beta_{i-1,p^{i-1}d} & \binom{2}{0}\beta_{i-1,p^{i-1}d}^2 & \hdots & \binom{p-1}{0}\beta_{i-1,p^{i-1}d}^{p-1} \\
    0 & 0 & \binom{2}{1}\beta_{i-1,p^{i-1}d} & \hdots & \binom{p-1}{1}\beta_{i-1,p^{i-1}d}^{p-2}\\
    \vdots  &&         &        & \vdots               \\
    0 & \cdots & \cdots  & 0      &\binom{p-1}{p-2}\beta_{i-1,p^{i-1}d} \\
    0 & \cdots & \cdots  & \cdots & 0
  \end{bmatrix}
\end{equation}
(we recall that $\beta_{i-1,n}=\PTr_{n}(\gamma_{i-1})$).  Then,
algorithm \titleref{alg:approximateas} finds the required solution.



\begin{algorithm}
  \caption{\alg{ApproximateAS}} 
  \label{alg:approximateas}
  \begin{algorithmic}[1]
    \REQUIRE $\eta\wrt\U_i$ such that~\eqref{eq:approximateAS} has a solution.
    \ENSURE $\mu\wrt\U_i$ solution of~\eqref{eq:approximateAS}.
    \STATE let $\eta_0 + \eta_1 x_i + \dots + \eta_{p-2} x_i^{p-2}=$ \titleref{alg:push-down}$(\eta)$;
    \FORALL {\label{alg:AAS:loop} $j\in[p-1,\ldots,1]$}
    \STATE let $\mu_j =
   \frac{1}{jT}\left(\eta_{j-1} -
     \sum_{h=j+1}^{p-1}\binom{h}{j-1}\beta_{i-1,p^{i-1}d}^{h-j+1}\mu_h\right)$;
   \ENDFOR
   \STATE return \titleref{alg:liftup}$(\mu_1 x_i + \ldots + \mu_{p-1} x_i^{p-1})$;
\end{algorithmic}
\end{algorithm}


\begin{theorem}
  \label{th:approximateAS}
  Algorithm \titleref{alg:approximateas} is correct and takes time $O(\Lift(i))$.
\end{theorem}

\begin{proof}
  Correctness is clear by Gaussian elimination.  For the cost
  analysis, remark that $\beta_{i-1,p^{i-1}d}$ has already been
  precomputed as a prerequisite for the iterated Frobenius and
  pseudotrace algorithms. Step~\ref{alg:AAS:loop} takes $O(p^2)$
  additions and scalar operations in $\U_{i-1}$; the overall cost is
  dominated by that of the push-down and lift-up by assumptions on
  $\Lift$.
\end{proof}

Writing the recursive algorithm is now straightforward. To solve
Artin-Schreier equations in $\U_0$, we use a naive algorithm based on
linear algebra, written \alg{NaiveSolve}.

\begin{algorithm}
  \caption{\alg{Artin-Schreier}}
  \label{alg:artin-schreier}
  \begin{algorithmic}[1]
    \REQUIRE $\alpha,i$ such that $\alpha\wrt\U_i$ and $\Tr_{\U_i/\F_p}(\alpha)=0$.
    \ENSURE $\delta\wrt\U_i$ such that $\delta^p-\delta=\alpha$.
    \STATE \label{alg:cou:base}if $i=0$, return \alg{NaiveSolve}$(X^p-X-\alpha)$;
    \STATE \label{alg:cou:pseudo} let $\eta =$ \titleref{alg:pseudotrace}$(\alpha, i,i-1)$;
    \STATE \label{alg:cou:push-beta} let $\mu=$ \titleref{alg:approximateas}$(\eta)$;
    \STATE \label{alg:cou:push-alpha} let $\alpha_0=$ \titleref{alg:push-down}$(\alpha-\mu^p+\mu)$;
    \STATE \label{alg:cou:rec} let $\Delta =$ \titleref{alg:artin-schreier}$(\alpha_0,i-1)$;
    \STATE \label{alg:cou:lift} return $\mu+$ \titleref{alg:liftup}$(\Delta)$.
  \end{algorithmic}
\end{algorithm}

\begin{theorem}\label{theo:AS}
  Algorithm \titleref{alg:artin-schreier} is
  correct and takes time
  \begin{equation}
    \label{eq:109}
    O(d^\omega + \Ptr(i))
    \text{.}
  \end{equation}
\end{theorem}
\begin{proof} 
  Correctness follows from the previous discussion.  For the
  complexity, let $\mathsf{AS}(i)$ be the cost for $\alpha\wrt\U_i$. The
  cost $\mathsf{AS}(0)$ of the naive algorithm is $O(\Mult(d)\log(p) +
  d^\omega)$, where the first term is the cost of computing $x_0^p$
  and the second one the cost of linear algebra.

  When $i\ge1$, step \ref{alg:cou:pseudo} has cost $\Ptr(i)$, steps
  \ref{alg:cou:push-beta}, \ref{alg:cou:push-alpha} and
  \ref{alg:cou:lift} all contribute $O(\Lift(i))$ and step
  \ref{alg:cou:rec} contributes $\mathsf{AS}(i-1)$. The most important
  contribution is at step \ref{alg:cou:pseudo}, hence $\mathsf{AS}(i) =
  \mathsf{AS}(i-1) + O(\Ptr(i))$. The assumptions on $\Lift$ imply that
  the sum $\Ptr(1) + \cdots + \Ptr(i)$ is $O(\Ptr(i))$.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Applying the isomorphism}

We get back to the isomorphism question. We assume that
$\lst{s}_i=(s_0,\dots,s_i)$ is known and we give the cost of applying
$\sigma_{\lst{s}_i}$ and its inverse.  We first discuss the forward
direction.

As input, $v \in \U'_i$ is written on the multivariate basis $\basis{B}'_i$
of $\U'_i$; the output is $t=\sigma_{\lst{s}_i}(v) \wrt \U_i$. As before,
the algorithm is recursive: we write $v=\Sigma_{j <p}
v_j(x'_0,\dots,x'_{i-1}) {x'_i}^j$, whence
\begin{equation}
  \sigma_{\lst{s}_i}(v)\ =\ \sum_{j
    <p} \sigma_{\lst{s}_i}(v_j) s_i^j\ =\ \sum_{j
    <p} \sigma_{\lst{s}_{i-1}}(v_j) s_i^j
  \text{;}
\end{equation}
the sum is computed by Horner's scheme.  To speed-up the computation,
it is better to perform the latter step in a bivariate basis, that is,
through a push-down and a lift-up.


\begin{algorithm}
  \caption{\alg{ApplyIsomorphism}} 
  \label{alg:applyisomorphism}
  \begin{algorithmic}[1]
    \REQUIRE $v,i$ with $v\in \U'_i$ written on the basis $\basis{B}'_i$.
    \ENSURE $\sigma_{\lst{s}_i}(v) \wrt \U_i$.
    \STATE if $i=0$ then return $v$;
    \STATE write $v=\Sigma_{j <p} v_j(x'_0,\dots,x'_{i-1}) {x'_i}^j$;
    \STATE let $s_{i,0}+\cdots+s_{i,p-1}x_i^{p-1}=$ \titleref{alg:push-down}$(s_i)$;
    \STATE for $j \in [0,\dots,p-1]$ let $t_j=$ \titleref{alg:applyisomorphism}$(v_j,i-1)$;
    \STATE let $t=0$;
    \STATE  for $j \in [p-1,\dots,0]$ let $t=(s_{i,0}+\cdots+s_{i,p-1}x_i^{p-1})t+t_j$;
    \STATE return \titleref{alg:liftup}$(t)$.
  \end{algorithmic}
\end{algorithm}

Given $t \wrt \U_i$, to compute $v=\sigma_{\lst{s}_i}^{-1}(t)$, we run
the previous algorithm backward. We first push-down $t$, obtaining
$t=t_0 + \cdots + t_{p-1}x_i^{p-1}$, with all $t_j \wrt
\U_{i-1}$. Next, we rewrite this as $t=t'_0+\cdots +
t'_{p-1}s_i^{p-1}$, with all $t'_j \wrt \U_{i-1}$, and it suffices to
apply $\sigma_{\lst{s}_i}^{-1}$ (or equivalently
$\sigma_{\lst{s}_{i-1}}^{-1}$) to each $t'_i$. The non-trivial part is
the computation of the $t'_j$'s: this is done by another application
of \titleref{alg:rur} in the extension
 $\U_i= \U_{i-1}[X_i]/(P_i)$, that we
shall call \titleref{alg:RUR-si}. We shall
discuss \titleref{alg:RUR-si} later, for the
moment we just state a result about its complexity.

\begin{lemma}
  \label{th:rur-s}
  Algorithm \titleref{alg:RUR-si} computes
  its output in time $O(p\Mult(p^id))$.
\end{lemma}

\begin{algorithm}
  \caption{\alg{ApplyInverse}} 
  \label{alg:applyinverse}
  \begin{algorithmic}[1]
    \REQUIRE $t,i$ with $t \wrt \U_i$.
    \ENSURE $\sigma_{\lst{s}_i}^{-1}(t)\in \U'_i$ written on the basis $\basis{B}'_i$.
    \STATE if $i=0$ then return $t$;
    \STATE let $t_0 + \cdots + t_{p-1}x_i^{p-1} =$ \titleref{alg:push-down}$(t)$;
    \STATE let $s_{i,0}+\cdots+s_{i,p-1}x_i^{p-1}=$ \titleref{alg:push-down}$(s_i)$;
    \STATE let $t'_0 + \cdots + t'_{p-1}X^{p-1} =$ \titleref{alg:RUR-si}$(t_0 + \cdots + t_{p-1}x_i^{p-1}, s_{i,0}+\cdots+s_{i,p-1}x_i^{p-1})$;
  \STATE return $\Sigma_{j < p}$ \titleref{alg:applyinverse}$(t'_j, i-1) {x'_i}^j$;
\end{algorithmic}
\end{algorithm}

\begin{proposition}\label{Prop:apply}
  Algorithms \titleref{alg:applyisomorphism} and
 \titleref{alg:applyinverse} are correct
  and both take time $O(i\Lift(i))$.
\end{proposition}
\begin{proof}
  In both cases, correctness is clear, since the algorithms translate
  the former discussion. As to complexity, in both cases, we do $p$
  recursive calls, $O(1)$ push-downs and lift-ups, and a few extra
  operations: for \titleref{alg:applyisomorphism}, these are $p$
  multiplications / additions in the bivariate basis $\basis{D}_i$ of
  Section~\ref{sec:level-embedding}; for \titleref{alg:applyinverse}, this is
  calling the algorithm
  \titleref{alg:RUR-si}.  The costs is
  $O(p\Mult(p^id))$ in both cases, which is in $O(\Lift(i))$ by
  assumptions on $\Lift$. We conclude as in Theorem~\ref{th:b-ifrob}.
\end{proof}


\paragraph{Rational univariate representation}
We describe now the algorithm to change from the basis $\basis{D}_i$
to the basis $\basis{D}_i'$, based on the rational univariate
representation. Unlike \titleref{alg:liftup}, this algorithm will not make use
of transposed subroutines.

We consider the algebra $\U_i[X_i]/(P_i)$. We have an element $s_i$,
its minimal polynomial $P'_i$, and we know that $s_i$ separates
$V(P_i)$. We want to express an element $t\in\U_i$ written on the
basis $\basis{D}_i$ as a rational fraction in $s_i$.

We first observe that the values of $\Tr_{\U_i/\U_{i-1}}$ over the
basis $1,x_i,\ldots,x_i^{p-1}$ are 
\begin{equation}
  \label{eq:110}
  \rho(1) = (0,\ldots,0,-1)
\end{equation}
by~\eqref{eq:pd}. Now, as in step~\ref{alg:rur:4} of
\titleref{alg:rur}, we need to compute $t\cdot\rho(1)$. By
writing down the multiplication matrix of $t$, we verify that
\begin{equation}
  \label{eq:111}
  t\cdot\rho(1) = (-t_{p-1}, -t_{p-2}, \ldots, -t_1, -t_0-t_{p-1})
  \text{.}
\end{equation}

Also observe that we need the inverse of the derivative of $P'_i$, as
in step~\ref{alg:rur:5} of \titleref{alg:rur}. Since $P'_i$ is Artin-Schreier,
this value is just $-1$. Finally, the algorithm
\titleref{alg:RUR-si} is as follows.

\begin{algorithm}
  \caption{\alg{RUR}$_{\lst{s}_i}$}
  \label{alg:RUR-si}
  \begin{algorithmic}[1]
    \REQUIRE $t_0,\ldots,t_{p-1}\wrt\U_{i-1}$.
    \ENSURE $t_0',\ldots,t_{p-1}'\wrt\U_{i-1}$ such that $t_0+\cdots+t_{p-1}x_i^{p-1}=t_0'+\cdots+t_{p-1}'s_i^{p-1}$.
    \STATE \label{alg:rur-s:1}let $\ell = t\cdot\rho(1) =  -\frac{t_{p-1}}{X_i^{p-1}} - \sum_{j=0}^{p-1} \frac{t_{p-1-j}}{X_i^j}$;
    \STATE \label{alg:rur-s:2}let $M = \frac{1}{T}\sum_{j=0}^{p-1}\frac{\ell(s_i^j)}{T^j}$;
    \STATE \label{alg:rur-s:3}let $V = P'_iM \bmod T^p$;
    \STATE return $-V$.
  \end{algorithmic}
\end{algorithm}

\begin{proof}[Proof of Lemma~\ref{th:rur-s}]
  Step~\ref{alg:rur-s:2} is the most expensive one. We use $p-1$
  multiplications in the bivariate basis $\basis{D}_i$ to compute
  $s_i,\ldots,s_i^{p-1}$, then we apply $\ell$ to each of them. These
  operations cost respectively $O(p\Mult(p^id))$ and
  $O(p^2\Mult(p^{i-1}d))$.

  Then, step~\ref{alg:rur-s:1} just costs one addition in $\U_{i-1}$
  and step~\ref{alg:rur-s:3} costs $O(d)$ additions in $\U_{i-1}$
  because $P_i'(T)=T^p-T'-\gamma_{i-1}'$.
\end{proof}

\begin{nota}
  We could have done better in step~\ref{alg:rur-s:2} by using
  transposed modular composition, but this would not influence the
  overall complexity of \titleref{alg:applyinverse}.
\end{nota}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection[Proof of the theorem]{Proof of Theorem~\ref{theo:main}}

Finally, assuming that only $(s_0,\dots,s_{i-1})$ are known, we
describe how to determine $s_i$. Several choices are possible: the
only constraint is that $s_i$ should be a root of
$X_i^p-X_i-\sigma_{\lst{s}_i}(\gamma'_{i-1})=X_i^p-X_i-\sigma_{\lst{s}_{i-1}}(\gamma'_{i-1})$
in $\U_i$.

Using Proposition~\ref{Prop:apply}, we can compute
$\alpha=\sigma_{\lst{s}_{i-1}}(\gamma'_{i-1}) \wrt\U_{i-1}$ in time
$O((i-1)\Lift(i-1)) \subset O(i\Lift(i))$.  Applying a lift-up to $\alpha$,
we are then in the conditions of Theorem~\ref{theo:AS}, so we can find
$s_i$ for an extra $O(d^\omega + \Ptr(i))$ operations.

We can then summarize the cost of all precomputations: to the cost of
determining $\lst{s}_i$, we add the costs related to the tower
$(\U_0,\dots,\U_i)$, given in
Sections~\ref{sec:fast-tower},~\ref{sec:level-embedding}
and~\ref{sec:pseudotrace-frobenius}. After a few simplifications, we
obtain the upper bound $O( d^\omega + \Ptr(i) + \Mult(p^{i+1} d)
\log(p)).$ Summing over $i$ gives the first claim of the theorem. The
second is a restatement of Proposition~\ref{Prop:apply}.

% Local Variables:
% mode:flyspell
% ispell-local-dictionary:"american"
% mode: TeX-PDF
% mode: reftex
% TeX-master: "../these"
% End:
%
% LocalWords:  Schreier Artin pseudotrace frobenius bivariate memoization
% LocalWords:  precomputed precomputation precompute precomputations Couveignes
