\section{Straight Line Programs}
\label{sec:stra-line-progr}

One can view arithmetic circuits as algorithms, where each node is an
elementary step. Then, the size of a circuit is a measure of
complexity in terms of number of elementary (algebraic)
operations. However, arithmetic circuits do not carry any information
about space complexity.

\index{straight~line~program}Straight line programs
(\index{SLP@see{straight~line~program}}SLP) allow to reason about both
space and time complexity: they can be seen as evaluation strategies
for arithmetic circuits, carrying information about registers to store
intermediate results. Informally speaking, they are programs that are
only made of a sequence of assignments (no branchings, no loops).
See~\cite{burgisser+clausen-shokrollahi} for formal definitions and
proofs.

\pdfmcone{More precisions on the algebraic RAM.}
We work in the algebraic \index{RAM~model}RAM model
of~\cite{kaltofen88:gcd}; this is to the classic RAM model what the
BSS model~\cite{BSS} is to the Turing machine. In a slightly
simplified way, an $R$-algebraic RAM (Random Access Machine) has a
memory, made of an infinite set of registers that can contain an
arbitrary element of $R$, and a CPU, that can perform arithmetic
operations on elements of $R$ and store the result in a register.

An SLP can be seen as a program for an algebraic RAM: its inputs are
initially stored in some registers, its instructions are executed in
order, and its outputs are to be read in some other registers. For
example, the program
\begin{equation}
  \label{eq:275}
  \begin{aligned}
    R_3 &\la R_1 + R_2\\
    R_3 &\la R_3 * 3\\
    R_2 &\la R_2 * R_3
  \end{aligned}
\end{equation}
expects is inputs in the registers $R_1$ and $R_2$, performs an
addition, a scalar multiplication and a multiplication, and stores its
output in the registers $R_2$ and $R_3$ (in our context, it is somehow
arbitrary to decide which are the input and output registers). In this
model, the time complexity of a SLP is given by the number of
instructions, the space complexity by the number of different
registers used.



\subsection{The BLS model}
\label{sec:bls-model}
In this section we study a particular family of \emph{linear straight
  line programs} introduced by Bostan, Lecerf and
Schost~\cite{bostan+lecerf+schost:tellegen} to study the transposition
principle. They consider straight line programs consisting uniquely of
the two operations
\begin{align}
  \label{eq:269}
  R_i &\la R_i + R_j
  \text{,}
  &\text{also written }
  R_i \overset{+}{\la} R_j
  \text{,}\\
  \label{eq:272}
  R_i &\la R_i * a
  \text{,}
  &\text{also written }
  R_i \overset{*}{\la} a
  \text{,}
\end{align}
where $R_i,R_j$ are registers and $a\in R$. 

\pdfmcone{Slightly rephrased.}
Such SLP's can compute the same morphisms as linear circuits
over the basis $\mathsf{BLS}$:
\begin{equation}
  \tag{$\mathsf{BLS}$}
  \label{eq:246}
  \mathsf{XOR}_1 :
  \begin{pmatrix}
    1 & 0\\1 & 1
  \end{pmatrix}
  \text{,}\quad
  \mathsf{XOR}_2 :
  \begin{pmatrix}
    1 & 1\\0 & 1
  \end{pmatrix}
  \text{,}\quad
  \rmul{a} :
  \begin{pmatrix}
    a
  \end{pmatrix}
  \text{ for $a\in R$.}
\end{equation}
In fact, consider a circuit $C$ over $(R,\mathsf{BLS})$ and let
$x_1,\ldots,x_n$ be its inputs. Allocate $n$ registers
$R_1,\ldots,R_n$ and initialize them to the values of
$x_1,\ldots,x_n$. Then, walk through $C$ in any topological order and
for any $\rmul{a}$ acting on $R_i$ issue the instruction
\begin{equation}
  \label{eq:249}
  R_i \lat a
  \text{,}
\end{equation}
for any $\mathsf{XOR}_1$ acting on $R_i$ and $R_j$ issue the
instruction
\begin{equation}
  \label{eq:255}
  R_i \lap R_j
  \text{,}
\end{equation}
and for any $\mathsf{XOR}_2$ acting on $R_i$ and $R_j$ issue the
instruction
\begin{equation}
  \label{eq:256}
  R_j \lap R_i
  \text{.}
\end{equation}
Observe that all the operators in \ref{eq:246} have the same input and
output arities, then circuits over \ref{eq:246} necessarily have the
same number of inputs and outputs. Hence, if a circuit has $n$ inputs
(and outputs), any topological order yields a straight line program
using $n$ registers by this evaluation strategy.  Inversely, it is
clear that any straight line program using only
instructions~\eqref{eq:269} and~\eqref{eq:272} can be represented by a
circuit over \ref{eq:246} having $n$ inputs (and outputs).

\pdfmcone{Added precision on the one-to-many correspondence.}  Thus,
we identify circuits over~\ref{eq:246} with such SLP's (note that the
identification is not one-to-one as different topological orders yield
different SLP's), and define the space complexity in the algebraic RAM
model of a circuit over~\ref{eq:246} as its number of inputs (and
outputs).

Finally, observing that $\mathsf{XOR}_1$ is the dual of
$\mathsf{XOR}_2$, we deduce that any circuit $C$ on $(R,\mathsf{BLS})$
has a dual circuit with the same space and time complexities in the
algebraic RAM model.


\begin{remark}
  Let $(L_1,\ldots,L_k)$ be a SLP on $n$ registers, where $L_i$ is one
  of the instructions~\eqref{eq:269} or~\eqref{eq:272}. By what we
  just said we can take as its dual the sequence
  $(\dual{L_k},\ldots,\dual{L_1})$, where $\dual{L_i}$ is defined as
  \begin{align}
    \label{eq:273}
    \dual{(R_i\overset{+}{\la}R_j)} &= R_j\overset{+}{\la}R_i\text{,}\\
    \dual{(R_i\overset{*}{\la}a)} &= R_i\overset{*}{\la}a\text{.}
  \end{align}
\end{remark}


\subsection{Linear straight line programs}
\label{sec:gener-stra-line}
\pdfmcone{"generic" -> "classic"}
The step from the SLP's we just defined to classic linear SLP's is
very small. In fact, all one has to do is simulate the instructions
\begin{align}
  \label{eq:257}
  R_i &\la R_j*a\quad\text{with $i\ne j$,}\\
  \label{eq:258}
  R_i &\la R_j + R_k\quad\text{with $i\ne j,k$.}
\end{align}
The first one can be simulated by the sequence
\begin{equation}
  \label{eq:259}
  \begin{aligned}
    R_i &\lat 0\text{,}\\
    R_i &\lap R_j\text{,}\\
    R_i &\lat a\text{;}
  \end{aligned}
\end{equation}
and the second one by
\begin{equation}
  \label{eq:260}
  \begin{aligned}
    R_i &\lat 0\text{,}\\
    R_i &\lap R_j\text{,}\\
    R_i &\lap R_k\text{.}
  \end{aligned}
\end{equation}
It is reasonable not to count multiplications by $0$, as these just
require to free some memory, than one sees that transposing linear
SLP's preserves the space complexity and loses a factor of at most two
on time complexity. However this is clumsy: one can do much better by
transposing directly the instructions~\eqref{eq:257}
and~\eqref{eq:258}.

\begin{definition}[Double use]
  We say that a register $R_i$ is \index{double~use}\emph{doubly used}
  in a sequence of instructions $(L_1,\ldots,L_n)$ if it appears on
  the right hand side of two instructions $L_i$ and $L_j$, and no
  instruction $L_k$ for $i<k<j$ is of the form~\eqref{eq:257}
  or~\eqref{eq:258}.
\end{definition}

The matrix of the instruction $R_i\la R_j*a$ is
$\left(\begin{smallmatrix}1&0\\a&0\end{smallmatrix}\right)$ in
general, but simply
$\left(\begin{smallmatrix}0&0\\a&0\end{smallmatrix}\right)$ if $R_j$
is not doubly used; in the second case, the transposition is
\begin{equation}
  \label{eq:262}
  \begin{aligned}
    R_j&\la R_i*a\text{,}\\
    R_i&\lat 0\text{.}
  \end{aligned}
\end{equation}
Similarly, the matrix of $R_i\la R_j+R_k$ is 
\begin{equation}
  \label{eq:265}
  \begin{pmatrix}
    0 & 0 & 0\\
    0 & 0 & 0\\
    1 & 1 & 0
  \end{pmatrix}
\end{equation}
if $R_j$ and $R_k$ are not doubly used; this transposes to
\begin{equation}
  \label{eq:263}
  \begin{aligned}
    R_j &\la R_i\text{,}\\
    R_k &\la R_i\text{,}\\
    R_i &\lat 0
  \end{aligned}
\end{equation}
(notice that a double use is introduced by this transposition).

\pdfmcone{Removed undefined "self-increment".}
By comparing this to equations~\eqref{eq:259} and~\eqref{eq:260}, one
sees that each double use of a register introduces a
$\overset{+}{\la}$ in the transposed code, and each addition
introduces a double use in the transposed code. This corresponds well
to the duality between $+$ and $\hub$.

In conclusion, one sees that the sum of additions and double uses
stays unchanged when transposing generic straight line
programs. Again, it is reasonable not to count multiplications by $0$
(in fact, they can be merged to the next assignment to the register).
Copies of registers like in~\eqref{eq:263} are still a problem in the
algebraic RAM model, but at a higher level of abstraction they can be
handled using references (or one can simplify the code by hand, if his
code has to run on an algebraic CPU!). Thus we can state the following
version of the transposition theorem for straight line programs.

\begin{theorem}[Transposition theorem]
  \label{th:tellegen-slp}\index{transposition~theorem}
  Any linear straight line program $S$ computing a function $f$ can be
  transformed in a new straight line program $\dual{S}$ computing
  $\dual{f}$.  $S$ and $\dual{S}$ use the same number of
  registers. The sum of the algebraic time complexity and the number
  of double uses of registers is the same for $S$ and $\dual{S}$.
\end{theorem}



\subsection{\texorpdfstring{$R$}{R}-algebraic transforms}
\label{sec:r-algebraic-transforms}
One rarely programs with straight line programs: to make transposition
really useful, we must transpose families of SLP's. Bostan, Lecerf and
Schost consider SLP's parameterized by integers and booleans.

\begin{definition}[algebraic transform]
  \label{def:algebraic-transform}
  Let $R$ be a ring, an \index{algebraic~transform}\emph{$R$-algebraic
    transform} is a program in the algebraic RAM model composed by the
  following constructs:
  \begin{itemize}
  \item linear algebraic assignments of the
    forms~\eqref{eq:269},~\eqref{eq:272},~\eqref{eq:257},~\eqref{eq:258};
  \item for loops with iterator ranging over a list of non-algebraic
    registers;
  \item conditionals with tests over non-algebraic registers;
  \item function calls, recursive function calls.
  \end{itemize}
  If the program is recursive, it must terminate on any valid input.
\end{definition}

\pdfmctwo{Explained what the algorithm does.}
By extension, we shall also call $R$-algebraic transform any algorithm
that can be expressed in this model. This is equivalent to consider
circuit families; for example, Algorithm~\ref{alg:r-algeb} corresponds
to a circuit family with parameter space $\N$, computing the map
\begin{equation}
  \label{eq:277}
  \begin{aligned}
    \N &\ra\hom(R^2,R^2)\text{,}\\
    n &\mapsto
    \begin{cases}
      \left(\begin{smallmatrix}1&0\\0&1\end{smallmatrix}\right) &\text{if $n=0$,}\\
      \left(\begin{smallmatrix}f_n&f_{n+1}\\f_{n-1}&f_n\end{smallmatrix}\right) &\text{if $n>0$ is odd,}\\
      \left(\begin{smallmatrix}f_{n-1}&f_n\\f_n&f_{n+1}\end{smallmatrix}\right) &\text{if $n>0$ is even,}
    \end{cases}
  \end{aligned}
\end{equation}
where $f_n$ is the $n$-th \index{Fibonacci~number}Fibonacci number.

\begin{algorithm}
  \caption{\label{alg:r-algeb}$R$-algebraic transform}
  \begin{algorithmic}
    \REQUIRE $a,b\in R$; $n\in\N$.
    \FOR{$i\in[1,\ldots,n]$}
    \IF{$i$ is odd}
    \STATE $a \lap b$;
    \ELSE
    \STATE $b \lap a$;
    \ENDIF
    \ENDFOR
    \STATE return $a,b$;
  \end{algorithmic}
\end{algorithm}


\pdfmctwo{Added exercise for the reader.}
It is clear that, for any value of the non-algebraic parameters, an
$R$-algebraic transform corresponds to a SLP, then the transposition
theorem can be applied to it. In practice, one leaves conditionals
untouched and reverses for loops; function calls (recursive or not)
are substituted by their transpose. For example,
Algorithm~\ref{alg:r-algeb} becomes Algorithm~\ref{alg:r-algeb-t}; we
let to the reader the verification that the transposed algorithm
computes the transpose of maps~\eqref{eq:277} for any $n$.

\begin{algorithm}
  \caption{\label{alg:r-algeb-t}Transposition of
    Algorithm~\ref{alg:r-algeb}}
  \begin{algorithmic}
    \REQUIRE $a,b\in R$; $n\in\N$.
    \FOR{$i\in[n,\ldots,1]$}
    \IF{$i$ is odd}
    \STATE $b \lap a$;
    \ELSE
    \STATE $a \lap b$;
    \ENDIF
    \ENDFOR
    \STATE return $a,b$;
  \end{algorithmic}
\end{algorithm}

Putting together the results of this section and the previous ones, we
can now state the transposition theorem for algebraic transforms.

\begin{theorem}[Transposition theorem]
  \label{th:tellegen-R-algeb}\index{transposition~theorem}
  Any $R$-algebraic transform $T$ computing a linear function
  $f$ can be transformed in an $R$-algebraic transform $\dual{T}$
  computing $\dual{f}$.  $T$ and $\dual{T}$ use the same number of
  registers. The sum of the algebraic time complexity and the number
  of double uses of registers is the same for $T$ and $\dual{T}$.
\end{theorem}

\begin{nota}
  Observe that some care must be taken when counting double uses in
  for loops: a single assignment in a for loop counts as $n-1$ double
  uses, where $n$ is the number of times the loop is repeated.
\end{nota}


\subsection{\texorpdfstring{$R$}{R}-algebraic algorithms}
\label{sec:r-algebr-algor}
The transposition theorem for algebraic transforms is an important
result that we shall use in the following chapters. However, if we
want to transpose the multiplication or the Euclidean division of
Section~\ref{sec:transp-algor}, we need to consider SLP's
parameterized by algebraic elements.

With a little of hand waving, the transposition theorem is applied to
algorithms parameterized by algebraic elements
in~\cite{bostan+lecerf+schost:tellegen}:
\begin{quote}
  ``Last, we will consider algorithms mixing linear and non-linear
  precomputations; the transposition principle leaves the latter
  unchanged.''
\end{quote}

We shall call \index{algebraic~algorithm}\emph{$R$-algebraic} any
algorithm that can be expressed in the algebraic RAM model, and that
terminates on any input. Formally, the way an $R$-algebraic algorithm
can be transposed is by \emph{partial
  evaluation}~\cite{consel+danvy93,riazanov+voronkov04,carette+kiselyov+shan09:jfp}.

\begin{definition}[Partial evaluation]
  Let $M,N$ be free $R$-modules (non necessarily finite). Let
  $A:M\times\pspace\ra N$ be an $R$-algebraic algorithm and let $p\in
  \pspace$. The \index{partial~evaluation}\emph{partial evaluation} of
  $A$ on $p$ is the algorithm
  \begin{equation}
    \label{eq:274}
    \begin{aligned}
      A_{p}:L&\ra N\text{,}\\
      x &\mapsto A(x;p)
      \text{.}
    \end{aligned}
  \end{equation}
\end{definition}

If for any $p\in\pspace$ the partial evaluation $A_{p}$ is
a straight line program, then we can apply the transposition theorem
to $A_{p}$. For example, if $M:R[X]\times R[X]\times \N\ra R[X]$ is
a polynomial multiplication algorithm, then for any $b\in R[X]$, the
\emph{transposed multiplication} is $\dual{M_{b,\deg b}}$, as in
Section~\ref{sec:transp-mult}.

There are many strategies to compute partial evaluations, we shall see
one in Section~\ref{sec:texttttransalpyne}. The simplest one is to
evaluate all the expressions that depend on $p$ and store them in
memory so that they can be used in $A_p$ as constants; this is similar
to the linearization of arithmetic circuits we saw in
Section~\ref{sec:multi}. Obviously, the cost in time complexity is
bounded by the time complexity of $A$; however, the cost in space
complexity is also bounded by the time complexity of $A$.

\begin{principle}[Transposition principle]
  \label{th:tellegen-princip}\index{transposition~theorem}\index{transposition~principle}
  Let $\pspace$ be an arbitrary set. Any $R$-algebraic algorithm $A$
  computing a family of linear functions $(f_p:M\ra N)_{p\in\pspace}$
  can be transformed in an $R$-algebraic algorithm $\dual{A}$
  computing the \emph{dual family}
  $(\dual{f}_p:\dual{N}\ra\dual{M})_{p\in\pspace}$. The algebraic time
  and space complexities of $\dual{A}$ are bounded by the time
  complexity of $A$.
\end{principle}

Notice, however, that in many practical instances of transposition,
the partially evaluated values constitute a negligible amount of the
space used by the algorithm. Thus, in practice, transposed algorithms
often have the same space \emph{and} time complexities as the original
ones; this is the case for all the transposed algorithms that appear
in this document.\footnote{Actually, the author is not aware of
  \emph{any} application of the transposition principle where space
  complexity is not preserved, although it is easy to artificially
  create examples that behave badly.}

\begin{nota}
  Is this the end of the story? Probably not. Umans and
  Kedlaya~\cite{kedlaya+umans08} have recently shown an example of
  non-algebraic algorithm that can be transposed with no loss in space
  and time complexity. This makes one wonder what the true limits of
  the transposition principle are.
\end{nota}



%%% Local Variables: 
%%% mode:flyspell
%%% ispell-local-dictionary:"american"
%%% mode: TeX-PDF
%%% mode: reftex
%%% TeX-master: "../these"
%%% End: 
