In this chapter we present a joint work with
Schost~\cite{df+schost10}. We study the
\index{automatic~transposition}\emph{automatic transposition} of
generic code (i.e.\ not limited to straight line programs).
Section~\ref{sec:autom-diff} has shown that this has applications in
automatic differentiation, and we will see other applications in the
next chapters.

By looking at a specific subproblem of automatic differentiation, our
goal is to be more efficient and more general. In particular, compared
to the existing implementations of AD tools, we want to:
\begin{itemize}
\item avoid unnecessary space overhead;
\item handle algebraic, rather than just numerical code;
\item handle advanced programming constructs, including recursion and
  algebraic data types;
\item transpose code parameterized by arbitrary algebraic variables.
\end{itemize}

In this chapter we shall abandon the algebraic RAM model we used in
Section~\ref{sec:stra-line-progr} and work on source code
transformation. Implementation details such as knowing what the cost
of copying variables is, shall be ignored: one can assume that a good
compiler will optimize most of these details. Hence, we shall assume
that Theorem~\ref{th:tellegen-R-algeb} really reflects the behavior of
the code we generate.


\section{Inferring linearity}
\label{sec:inference}
\lstset{language=haskell}

By looking at Section~\ref{sec:transp-algor} one sees that often we
want to transpose families of $R$-algebraic algorithms parameterized
by algebraic elements (e.g., we want to transpose the code that for
any $a\in R$ evaluates the map $b\mapsto ab$). This is also necessary
in automatic differentiation, when the code for $\diff_x f$ not only
depends on $\diff x_1,\ldots,\diff x_n$, but also on $x$.

The next section will address the question of how to transpose such
code. This section, instead, asks the question: can a compiler guess
by itself which inputs to a function are parameters, and which are
linear arguments?

The answer is yes. We show how the type system of common statically
typed functional languages can be extended to automatically infer all
the possible \index{linearization}\emph{linearizations} of a computer
program. We first present the non-commutative case, which can be fully
expressed inside the Haskell type system, then we discuss how to
extend to the commutative case.

\paragraph{Linears, scalars}
\label{sec:linears-scalars}
Suppose we have defined some data type \lstinline{R} representing
elements of a ring $R$ together with the usual constants (say
\lstinline{zeroR}, \lstinline{oneR}, etc.), arithmetic operations (say
\lstinline{plus}, \lstinline{times}, etc.), tests and so on. To
simplify, we assume --as usual in algebraic complexity theory-- that
the type \lstinline{R} is isomorphic to $R$, i.e.\ the elements of $R$
can be represented exactly, the operations do not introduce any
rounding error, etc.

For any term involving elements of type \lstinline{R} we would like
the type system to tell us whether its outputs are linear in its
inputs. For example the term
\begin{lstlisting}
  \x y -> plus x y
\end{lstlisting}
has type \lstinline{R -> R -> R}, but we would like the type checker
to also output something like $\ell\ra\ell\ra\ell$ ($\ell$ for
\index{linear~input}\emph{linear}) telling that the term is a
(curryfied) left module homomorphism from $R^2$ to $R$. For
consistency, we want to view constants as mappings from $R^0$ to $R$,
thus for the term \lstinline{zeroR} we want the type checker to
compute something like $0\ra\ell$, that we simply write as $\ell$.

Now, what do we expect about \lstinline{oneR} or \lstinline{times}?
The former is the mapping $\bom\mapsto1$, which is not a module
homomorphism; then, by analogy with
Definition~\ref{def:linearization}, we want the type checker to output
something like $0\ra s$, or simply $s$ ($s$ for
\index{scalar~input}\emph{scalar}). The second can be made into a
linear mapping by \emph{fixing} its second argument (remember that for
the moment we are restricting to left modules) as we did in
Section~\ref{sec:multi}; thus we expect the type checker to output
$\ell\ra s\ra\ell$, meaning that
\begin{lstlisting}
  \x -> times x y
\end{lstlisting}
is a left module homomorphism $R\ra R$ for any \lstinline{y::R}.

Finally consider the following term
\begin{lstlisting}
  z x n = if n <= 0 then zeroR else plus x (z x (n-1))
\end{lstlisting}
as before we expect something like $\ell\ra\N\ra\ell$, meaning that
\begin{lstlisting}
  \x -> z x n
\end{lstlisting}
is a homomorphism $R\ra R$ for any integer \lstinline{n}.

Observe that in order to make a correct inference about a term such as
\begin{lstlisting}
  \x y -> times x (plus y y)
\end{lstlisting}
we must also admit for any of the previous cases the possibility where
everything is a scalar, so that from the hypothesis that
\lstinline{plus} has type $s\ra s\ra s$ we can deduce the correct type
$\ell\ra s\ra\ell$ for the term above. Summarizing, we would like to
have two types \lstinline{L} and \lstinline{S} such that the following
equations hold
\begin{lstlisting}
  plus :: L -> L -> L
  plus :: S -> S -> S
  times :: L -> S -> L
  times :: S -> S -> S
  zeroR :: L
  zeroR :: S
  oneR :: S
\end{lstlisting}

\pdfmcone{Better with newtypes, thanks to Mathieu.}
If we define \lstinline{L} and \lstinline{S} as wrappers around
\lstinline{R}
\begin{lstlisting}
  newtype L = Lin R
  newtype S = Sca R
\end{lstlisting}
then, using Haskell type classes
\cite{Walder+Blott-ad-hoc-polymorphism}, we can conveniently express
all the equations above as
\begin{lstlisting}
  class Ring r where
    zero :: r
    (<+>) :: r -> r -> r
    neg :: r -> r
    (<*>) :: r -> S -> r
\end{lstlisting}
together with the obvious \lstinline{instance} definitions (see the
example in Appendix~\ref{cha:line-infer-karats}). 

\pdfmcone{Better terminology, thanks to Mathieu.}  For our
inference to work, it is important that \lstinline{L} be an abstract
data type with only the above functions in its interface. On the other
hand, any other function acting on \lstinline{R} can be wrapped inside
a function acting on \lstinline{S} as, for example,
\begin{lstlisting}
  one = Sca oneR
  (Sca a) == (Sca b) = a == b
\end{lstlisting}
or, simply, using a \lstinline{deriving} clause in the declaration of
\lstinline{S}
\begin{lstlisting}
  newtype S = Sca R deriving (Eq)
\end{lstlisting}

If we restrict to terms that do not use the type constructor
\lstinline{Lin}, then we can show that the semantic of a term with
type
\begin{lstlisting}
  L->...->L->L
\end{lstlisting}
is a left module homomorphism.  \pdfmcone{More details on the
  simply typed lambda calculus, as suggested by Mathieu.}  The proof
for the full language would be too long, thus we restrict to a simply
typed $\lambda$-calculus with constants. Its terms are defined by the
following grammar
\begin{equation}
  \label{eq:lambda}
  t ::= c \;|\; x \;|\; t_0 t_1 \;|\; \lambda x . t  \text{ ,}
\end{equation}
where $x$ are identifiers and $c$ are constants; its types are
defined by the grammar
\begin{equation}
  \label{eq:77}
  \tau ::= \ell \;|\; s \;|\; \beta \;|\; \tau \ra \tau
  \text{ ,}
\end{equation}
where $\beta$ are the usual base types (integers, booleans, etc.). If
$\Gamma$ is a type environment, by $\Gamma\vdash t::\tau$ we mean that
the term $t$ has type $\tau$ in $\Gamma$.  The semantic of our
calculus is the usual one, based on $\beta\eta$-reduction.

\pdfmcone{Careful about lists and tuples, thanks Mathieu!}  We
suppose all the constants above are defined, plus the usual constants
for the other base types; observe that our grammar forbids type
constructors altogether (including \lstinline{Lin} and
\lstinline{Sca}). In this context we use the type names $\ell$ and $s$
in place of the Haskell types \lstinline{L} and \lstinline{S} defined
above. For simplicity, we shall also assume that lists and tuples are
not part of the types of our language; see the end of this section for
a discussion about them.

\begin{definition}[Flipper]
  \pdfmcone{Defined the flipper and changed the proof of the
    lemma, thanks to LÃ©o.}  Let $\tau$ be the type
  \begin{equation}
    \label{eq:58}
    \tau = \alpha_0\ra\alpha_1\ra\cdots\ra\alpha_n
  \end{equation}
  with $\alpha_n$ not a function type. Let $I\subset[0,\ldots,n-1]$
  such that $\alpha_i\ne\ell$ if and only if $i\in I$, and let
  $m=\card{I}$.  The \emph{flipper} for $\tau$, denoted by $\flip_\tau$,
  is the term
  \begin{equation}
    \label{eq:60}
    \flip_\tau = \lambda t.\lambda x_{i_1}.\ldots.\lambda x_{i_m}.
    \lambda x_{j_1}.\ldots.\lambda x_{j_{n-m}}.tx_0\cdots x_{n-1}
    \text{,}
  \end{equation}
  with $i_1,\ldots,i_m\in I$ and $j_1,\ldots,j_{n-m}\in\bar{I}$.
\end{definition}

\begin{lemma}
  \label{th:lininference}
  Let $\Gamma\vdash t::\tau$ be a term, let $m,n\ge0$, and let
  $\Gamma\vdash \flip_{\tau}t::\sigma$ with
  \begin{equation}
    \label{eq:264}
    \sigma=\alpha_1\ra\alpha_2\ra\cdots\ra\alpha_m\ra\underbrace{\ell\ra\cdots\ra\ell}_{\text{$n$ times}}\ra\beta
    \text{,}    
  \end{equation}
  with $\alpha_i\ne\ell$ and $\beta$ not a function type. Let
  $\Delta_i\vdash s_i::\alpha_i$ for $1\le i\le m$. The semantic of
  \begin{equation}
    \label{eq:270}
    \Gamma,\Delta_1,\ldots,\Delta_m\vdash \flip_\tau ts_1\cdots s_m
  \end{equation}
  is
  \begin{enumerate}
  \item\label{item:1} a constant function if $\beta\ne\ell$,
  \item\label{item:2} a module homomorphism $R^n\ra R$ if $\beta=\ell$,
  \end{enumerate}
  assuming the free variables in $t,s_1,\ldots,s_m$
  satisfy~\ref{item:1} or~\ref{item:2}.
\end{lemma}
\begin{proof}
  We distinguish the following cases.
  \begin{itemize}
  \item $\Gamma\vdash c$. All the constants satisfy
    either~\ref{item:1} or~\ref{item:2}. We just work out $0$ and $+$
    defined above and leave the others to the reader; in both cases
    $\flip_\tau c=c$ up to $\beta\eta$-conversion.
    \begin{itemize}
    \item $\Gamma\vdash0::\ell$ is the map $\bom\mapsto 0$, thus a
      (constant) morphism.
    \item $\Gamma\vdash0::s$ is the map $\bom\mapsto 0$, thus a constant
      (morphism).
    \item $\Gamma\vdash+::\ell\ra\ell\ra\ell$ is the map $a,b\mapsto
      a+b$. A morphism.
    \item $\Gamma\vdash+::s\ra s\ra s$. Take any $a::s$ and $b::s$, then
      $\Gamma\vdash a+b::s$ is a constant.
    \end{itemize}
  \item $\Gamma,x::\alpha\vdash x::\alpha$. The claim follows because
    $x$ is free.
  \item $\Gamma\vdash t_0t_1::\tau$. This is the only real case
    to prove. We distinguish two cases:
    \begin{itemize}
    \item $\Gamma\vdash t_1::\ell$, then, by induction its semantic is
      a morphism $0\ra R$ (because it is $\beta\eta$-equivalent to
      $\flip_\ell t_1$). 

      Let $\Gamma\vdash \flip_\tau t_0t_1::\sigma$, with $\sigma$ as in
      Eq.~\eqref{eq:264}, and let $\Delta_i\vdash s_i$ for $1\le i\le
      m$ be as in the hypothesis.  Let $\Gamma\vdash t_0::\tau_0$ and
      $\Gamma\vdash\flip_{\tau_0}t_0::\sigma_0$, then by induction
      \begin{equation}
        \label{eq:271}
        \Gamma,\Delta_1,\ldots,\Delta_m\vdash t_0'\eqdef\flip_{\tau_0}t_0s_1,\ldots,s_m
      \end{equation}
      is either a morphism $R^{n+1}\ra R$ or a constant function. In
      the first case $t_0't_1$ is a morphism $R^{n'}\ra R$, in the second
      case it is a constant function; in both cases
      \begin{equation}
        \label{eq:74}
        \flip_\tau(t_0t_1)s_1\cdots s_m\xleftrightarrow{\beta\eta}t_0't_1        
      \end{equation}
      and the claim follows.
    \item $\Gamma\vdash t_1::\alpha$ with $\alpha\ne\ell$. Then the
      claim follows directly by induction on $t_0$ and
      $\beta\eta$-conversion, by choosing $s_1=t_1$.
    \end{itemize}
  \item $\Gamma\vdash \lambda x.t::\alpha_1\ra\alpha_2$. By induction
    $\Gamma,x::\alpha_1\vdash t::\alpha_2$ satisfies~\ref{item:1}
    or~\ref{item:2} (assuming $x$ does). We distinguish two cases
    \begin{itemize}
    \item $\alpha_1\ne\ell$, then
      \begin{equation}
        \label{eq:73}
        \lambda x.\flip_{\alpha_2}t\xleftrightarrow{\beta\eta}\flip_{\alpha_1\ra\alpha_2}(\lambda x.t)
        \text{;}
      \end{equation}
    \item $\alpha_1=\ell$, then
      \begin{equation}
        \label{eq:76}
        \lambda x.\flip_{\alpha_2}ts_1\cdots s_m\xleftrightarrow{\beta\eta}
        \flip_{\alpha_1\ra\alpha_2}(\lambda x.t)s_1\cdots s_m
        \text{.}
      \end{equation}
    \end{itemize}
    In both cases, $\lambda x.t$ satisfies~\ref{item:1}
    or~\ref{item:2} accordingly.
  \end{itemize}
\end{proof}

\begin{proposition}
  Let $t:\tau$ be a closed term, let $n\ge0$ and let
  \begin{equation}
    \tau=\underbrace{\ell\ra\cdots\ra\ell}_{\text{$n$ times}}\ra\beta
    \text{,}    
  \end{equation}
  with $\beta$ not a function type. Then, the semantic of $t$ is
  \begin{enumerate}
  \item a constant function if $\beta\ne\ell$,
  \item a module homomorphism $R^n\ra R$ if $\beta=\ell$.
  \end{enumerate}
\end{proposition}

By the proof, it should be now clear why we forbid the type
constructor \lstinline{Lin}. In fact, introducing a term as
\lstinline{Lin oneR :: L} tricks the proof (the type checker) by
making it believe that the function $\bom\mapsto 1$ is a morphism.


\paragraph{The commutative case}
\label{sec:commutative-case}
In the commutative case we shall add a second multiplication operator
allowing multiplication on the left by a scalar
\begin{lstlisting}
  class Ring r => CommRing r where
    (>*<) :: S -> r -> r 
\end{lstlisting}
but this would force the user to chose between the two operators any
time he multiplies two elements of $R$. To avoid this we need to
overload the operator \lstinline{(<*>)} with both type signatures, a
technique sometimes called \emph{ad-hoc} polymorphism
\cite{strachey00}, but this is not possible in the Haskell type system
since the two types are contradictory.  To make it possible we need to
extend the type inference algorithm: our idea is not new, but it has
been rarely implemented because it is not practical for solving
generic \emph{ad-hoc} polymorphism; it perfectly fits the needs of our
special case, though.

First observe that type classes can be translated to ordinary types of
the Hindley-Milner type system as explained in \cite[$\S
4$]{Walder+Blott-ad-hoc-polymorphism}, thus it suffices to modify the
classic type inference algorithm
\cite{Damas+Milner,Cardelli:Typechecking}. Second, observe that there
is some redundancy between the two signatures of \lstinline{(<*>)} and
that a more concise version is
\begin{lstlisting}
  (<*>) :: Ring r => r -> S -> r
  (<*>) :: S -> L -> L
\end{lstlisting}

A review of the Hindley-Milner algorithm and its implementation can be
found in \cite{Cardelli:Typechecking}. The idea is to first assign
type variables to terms, then solve type equations by unifying them.
In our generalization, instead of handling a single unification, we
keep a list of possible unifications: when a type equation implies
that a certain unification is not acceptable, the unification is
discarded from the list; if the list gets empty the term cannot by
typed and an error is returned, otherwise any unification in the list
is valid and is returned.

In practice, the only term that makes the list of unification grow is
\lstinline{(<*>)}: any time an equation involving it has to be solved,
the list of unifications potentially doubles. This exponential
increase is the reason why this solution is not practical to solve
generic \emph{ad-hoc} polymorphism; but in our case we really are
interested in knowing all the possible types of a term because each of
them gives rise to a different linearization and, hence, to a
different transposition.

\paragraph{Modules}
\label{sec:modules}
Finally we remark that by allowing tuples and lists,
Lemma~\ref{th:lininference} can be generalized to morphisms $R^m\ra
R^n$ and even to infinite dimensional modules using lazy
lists. Elements of type \lstinline{L}, \lstinline{[L]},
\lstinline{(L,L)}, etc. share a common pattern: they can be viewed as
$R$-modules. It is convenient to summarize their properties in an
unique interface\footnote{We make use of some experimental modules of
  Haskell: this codes needs the flags
  \lstinline{-XMultiParamTypeClasses},
  \lstinline{-XFunctionalDependencies} and
  \lstinline{-XFlexibleInstances} in order to work.}
\begin{lstlisting}
  class Ring r => Module m r | m -> r where
    zeroM :: m
    (<<*) :: m -> S -> m
    (>>>) :: m -> Integer -> r
    (<<<) :: r -> Integer -> m
    (<++>) :: m -> m -> m
    add :: m -> m -> Integer -> m
    add a b n = foldl (<++>) zeroM
                [((a>>>i) <+> (b>>>i))<<<i | i <- [1..n]]
\end{lstlisting}

Instances of this class represent free $R$-modules: \lstinline{zeroM}
is the zero element, \lstinline{(<<*)} is scalar multiplication,
\lstinline{(<++>)} is addition, \lstinline{(<<<)} and
\lstinline{(>>>)} are canonical injections and projections.

This interface adds nothing to the linearity inference system, but we
will need it in Section~\ref{sec:texttttransalpyne}.  Also notice the
presence of the operator \lstinline{add} that performs addition up to
a truncation order, it is of no great importance in this section, but
for efficiency reasons we will eventually prefer it to plain addition.

A fully worked Haskell example of the ideas presented in this section
(without the extension to the commutative case) is given in
Appendix~\ref{cha:line-infer-karats} where we implement Karatsuba
multiplication of polynomials in $\Z[X]$.


% Local Variables:
% mode:flyspell
% ispell-local-dictionary:"american"
% mode:TeX-PDF
% mode: reftex
% TeX-master: "../these"
% End:
%
